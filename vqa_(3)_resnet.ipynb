{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K-Divyasri/vqa/blob/main/vqa_(3)_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dependencies"
      ],
      "metadata": {
        "id": "-SYnhf242Mju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "3olhAYCUGxb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8317b625-33ff-4988-a477-651ab5abc0af"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.4)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed tensorflow-2.15.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- imports"
      ],
      "metadata": {
        "id": "Ztylycys2IXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Flatten, BatchNormalization, Dropout\n",
        "from tensorflow.keras.layers import multiply\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "4yX9_sfgKNjL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "b18ACNllCuFa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Global configuration variables"
      ],
      "metadata": {
        "id": "aewZWPHT2Soz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '/content/drive/Shareddrives/FYP_div' ##\n",
        "MODEL_PATH = BASE_PATH + \"/out/basic_model1.h5\"\n",
        "QUESTION_COUNT = 20\n",
        "CONVERT_TO_LOWERCASE = True\n",
        "QUESTIONS_TO_BE_INCLUDED = 18\n",
        "TEST_SIZE = 0.3\n",
        "WORD_VECTOR_MODEL = 'glove-wiki-gigaword-300'\n",
        "IMG_DIMN = 4096\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 15\n",
        "LR = 0.001\n",
        "DEMO_COUNT = 15\n",
        "USE_CHECKPOINT=False\n",
        "\n",
        "def sync_gdrive(init=False):\n",
        "  if not init: drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=init)\n",
        "\n",
        "sync_gdrive(init=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIV1hIJ9_WIE",
        "outputId": "ff50f89a-53ce-4199-d214-62b466670800"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Downloading the dataset (Run only once)"
      ],
      "metadata": {
        "id": "1eeeE7OG2bY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Downloading ImageCLEF Dataset\n",
        "\n",
        "# !gdown 1jTyLWwcHzbLpWjSNwmgiiavXDjuQe5y7 # Development dataset\n",
        "# !gdown 1PQPiOkyfQrLJ5wWxkHZy_FYIdL9hXiMl # Testing dataset\n",
        "\n",
        "# !unzip ImageCLEF\\*\n",
        "# !rm -rf *.zip\n",
        "\n",
        "# !mkdir $BASE_PATH/dataset\n",
        "# !mkdir -p $BASE_PATH/out/trial1\n",
        "# !mv ImageCLEF* $BASE_PATH/dataset\n",
        "\n",
        "# !mv $BASE_PATH/dataset/*Dev* $BASE_PATH/dataset/dev\n",
        "# !mv $BASE_PATH/dataset/*Test* $BASE_PATH/dataset/test\n",
        "\n",
        "# sync_gdrive()"
      ],
      "metadata": {
        "id": "lbysNpwm_j31"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(BASE_PATH + '/dataset/dev/gt.json', \"r\") as json_file:\n",
        "    gt = json.load(json_file)"
      ],
      "metadata": {
        "id": "4f5EuIwz-7_n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on below data analysis, 2 data points do not make sense. The below code-block has been edited to exclude these 2 images.\n",
        "\n",
        "- `cl8k2u1rr1gm30832fz9x3vru`  \n",
        "![1.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCACwApMDASIAAhEBAxEB/8QAHQABAAICAwEBAAAAAAAAAAAAAAUHBAYCAwgBCf/EAEkQAAEDAwIEAgcEBgYIBwEAAAEAAgMEBREGEgcTITEUQRYiUVVhlNEVIzJxJEJSU4GSCDM2kaGyJUNydoKDs8EXNDVUc3Sxwv/EABoBAQADAQEBAAAAAAAAAAAAAAABAgMEBQb/xAA3EQACAQMDAgQEBQMDBQEAAAAAAQIDESEEEjFBUQUUYXEigZGhEzKxwdEGYvAWQuEjUoKi8XL/2gAMAwEAAhEDEQA/APz6REXWZGdYrLcNSXu36etMIlrrnVRUdMwuDQ6WRwa0EnoOpHUqc4iaC/8ADu9CwS6ssV8q4w9tV9kzSSsppWvLXRPc9jPWBHkCFtv9Hy08PqvWlhr9Rayr7ZfKa+0ZtlDFbjLDVPEjDGHzh2YgX+qTsdgdevZafxRfXycS9VyXShioqx96rXVFPFLzGRSGd5c1r8DcAcjOBnvgdlE3ZxS9b/a36u/yJirqT9v3/g1hERSQEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEUvyIP3LP5QnIg/cs/lCttBz0RqCLSes7DqmelNVHZ7lTV74AcGRsUrXluT2ztwpvi3XaHvWr6zU2h7xdqyC91E9wqYrlRMp5aWaWVzjFlj3NeBn8Qx+SgnQQbW/cx9v2R7V85EH7ln8oSUG7el/va/6ILF/X9v8A6yIRS/Ig/cs/lCciD9yz+UJtBEIpfkQfuWfyhblw4sVFeKmppX2ulqpXvhjibJEw+s4uGAXdBk4Vo09ztclRcmoxV2ytkV/VWhrfR0/i6nTVuZFznU+7kRH7xvcdP/3sstvDEupBWt0hQmIx80DkQ7yz27Pxf4KzpRSu5o6oeH6ypJwhRk2ldra7pd+OPU87Iry9HdP+4rf8qz6J6O6f9xW/5Vn0WnlX3OK5RqK8vR3T/uK3/Ks+iejun/cVv+VZ9E8q+4uUaivqn0daqqCoqqfTlA+KlaHTO8NH6oJwPJdMemtPySNj+xba3cQNzqZgA+J6dlHlm8XNHCcVFtNKXHrm2PngotFflz0fp23VjqVtvs1WGgHm08EbmHI9uFi+jun/AHFb/lWfRI6ZyV0xVpyozdOas1hlGory9HdP+4rf8qz6LJoNGWu5yPhotOW6R8cbpXDkRNwwdz1AR6ZxV2xThOtNU6cW2+Est/IoRFfVXo600IgNVp23M8RE2eP9HiO5jux6Dp281j+jun/cVv8AlWfRFpm1dMicZUpbZpp+pRqK8vR3T/uK3/Ks+ikrTw7p77BcKm16Xt00VqpnVlW4xQs5cIIBd62N3UjoMn4JLT7FulJJERTk9sVdnntF6UPBu4DUlLpD0Lt/2tWwtngp9tP67CwvB3Z2j1QTgnP8VH03DqCrtdzvNPpa3vo7M6JtbJyYRyjI4tZ0PV2XAj1QceeFmqcHlTXTr3dl9Xhd3g0dGquYv6el/wBM+2Tz2ivL0d0/7it/yrPono7p/wBxW/5Vn0WvlX3MblGory9HdP8AuK3/ACrPono7p/3Fb/lWfRPKvuLlGory9HdP+4rf8qz6J6O6f9xW/wCVZ9E8q+4uUaivL0d0/wC4rf8AKs+iejun/cVv+VZ9E8q+4uUaivL0d0/7it/yrPono7p/3Fb/AJVn0Tyr7i5RqK96XS2n6qqhpjabVDzZGx8yWnY1jMnG5xx0A7krP1NoHTmnLzPaI49O3VsO3FVb2RywSZaD6rtvXGcH4hVdC0lC+WXUZOLmuF+555RXl6O6f9xW/wCVZ9E9HdP+4rf8qz6K3lX3KXKNRegLHoGk1JdILLZNL0FVW1JcIom08TS7DS49TgdgT38lhv01YY3ujfYbeHNJaR4ZnQj+Cjy/xbdyuWtLbvtji5RaK8vR3T/uK3/Ks+iejun/AHFb/lWfRT5V9ytyjUV5ejun/cVv+VZ9E9HdP+4rf8qz6J5V9xco1FeXo7p/3Fb/AJVn0T0d0/7it/yrPonlX3FyjUXoKz8PqbUElVFZ9K0NS6jppKycNp4hshZjc/rjtkduqj/R3T/uK3/Ks+ihae7cVJXRZqSipNYZRqK8vR3T/uK3/Ks+iejun/cVv+VZ9FPlX3K3KNRXl6O6f9xW/wCVZ9E9HdP+4rf8qz6J5V9xco1FeXo7p/3Fb/lWfRPR3T/uK3/Ks+ieVfcXKNRXl6O6f9xW/wCVZ9F20ultP1VVDTG02qHmyNj5ktOxrGZONzjjoB3JTyr7k3KIRehtTaB05py8z2iOPTt1bDtxVW9kcsEmWg+q7b1xnB+IUX6O6f8AcVv+VZ9FWGn/ABIqUXhlpxlTk4S5RRqK8vR3T/uK3/Ks+izrHoGk1JdILLZNL0FVW1JcIom08TS7DS49TgdgT38lMtNtTlKSSREbyajFXbPP6K/7XoShvN3p7FbdM2+auqphTwxciJu6QnAGXYA6+ZOFj1ekrPQVc1DV6et7J6eR0UjfDxna9pwRkDB6jyUeXzt3K4cZJXa9CiEV5ejun/cVv+VZ9E9HdP8AuK3/ACrPoreVfcrco1FeXo7p/wBxW/5Vn0UxfuHenLHS2uqadM3H7SpRUmOiDJH0pOPu5htGx/Xt+fsVXp9rUW+S8YuScl05/Q86ory9HdP+4rf8qz6J6O6f9xW/5Vn0VvKvuUuUaivL0d0/7it/yrPono7p/wBxW/5Vn0Tyr7i5RqK8vR3T/uK3/Ks+i2PS/But1nBJUac0jaapsUnKc1z6WJ5fjOAyRzXO6HyBVKlGNKO6pJJd3gvThOrLbTTb7LJ5qReirdwvfddSN0jR6OozeHSPi8LLTxROD2AucCX4AwGk9Suq5cOYbRbaC8XDS1uipLoZhSScqF3MMT9knQZIw7p1Az5ZUfhQuo71d8fO/wDD+j7FnSqJOTi7LnHHueekV5ejun/cVv8AlWfRVX4am/8Abx/yhKlF07XZmncgUUw+ngDiBCz+UL5yIP3LP5Qs9pJEIpfkQfuWfyhORB+5Z/KE2giEUvyIP3LP5QibQc0RFcHJ34W/l/3XFcnfhb+X/dcUAREQBb3wrq5KCtqa6FrTJTy08rQ4dCWlxGfh0WiLdeHIP+kD/wDF/wD2tKSvNJkxnKnJTi7NFi11+rLhReAmjhbH4l9VloIO9+cjqe3VbTuceKMYLiQ0taBnsOR2WiLbbRq+OaRzbuylgqW0zooLkIC6WN+MNLu+enTIGVbUUnGLdNdGvrb+D6bwbxGNatGOsq2anSkm+Gqe74W8JX3YbxjLXJgV9NTt026obBGJftaWPeGjdtDAQ3Ps+CnxaLW/UrBJS0scNPaG1W17NsReAPWeB3HXJ/JavbdQVNsppKA0tHW0z5Oby6mIvaH4xuHUHJC5DU91F4dei+MzObyyws+7MeMbNv7OFWVGq7pPv172NdP4p4dS2VKkbtuF1tWNrld363usdepK6kjs0loZMyptDq+OcAC3x7A6IjqC3tkHzUlOy3VwqGWWisFZSGAmKBo5VYAG9XZIySOp7rVbjqCpr6eOkjpKSigik5wjpYtgL8Y3Ekk5A+KzH60uDnSVDbdbmVcsZjdVsgIlORgnOcZx8FV6ersSXOevt/mODoh4zoPMVJTsotRTahl2Ur2bb7pWkrSsm7WzJaau726du4FBQHwkEWM0zTzPWP8AWftfxWvsvcjK99f9m20l7NnKNK0xDt1DewPTv+a+Wi91Nn57YoKeeKpZslhnYXMcAcjsQen5rmy9xsrJKv7DtjhI0N5LonctuPMDdnP8VsqO2cpbb3/4PMqeJ+Y0unpus4yp3TxfrJpq3o0iQv8Ab6R+tjbo4WQwSz08ZZE0NDQ5rM4A6DuVJDwF2uN4sJs1BTw0cM5glihxK10ZwCXd3Z8wVB3TVFRdXmeS20EFSXtf4iGNwkBbjHUuPsC51Wr6+qhqGCioIZqtnLnqIoNssjT3BOcdfPAWTo1XCMeqVueuMndHxLw6lqK9RSvGc3K2z80Xu+DPHK9PoiSp6qktto0+RZrdUPrTKJnzwB7nATEd/bg9/gFhXOpOl9S3OK1wx8tzXwBrwSGteATjBHbyUXJd6mSnt9M5kW22lxiIBy7c/cd3Xr19mF13S4z3avmuNS1jZJ3bnBgIaOmOmSfYtY0Hvblw91/m8fY4dT4vTlp4xoO04fhbXZK22ntnn1nZ+vJ9uFznuXhueyNvhKdlMzYCMtbnBOT36qZ1Rqm1X+02O32/SNvtE1qpuRU1NNjfXPw0cyTDR19UnqXHLj1WtrZNUa7uurbTY7PcKO3ww2Cm8LTOpoSx8jMNGZCScn1R2AGSenVbuKVkl1/k+fqVZ1pupUd21z9CDhZROppXzTPbM3HLaB0d/gpLT+q7jpujvFFQw0z2XuhdQVBla4lsZcHZZgjDstHfI+ChUUzhGpHbJXRWEnCSlF5RbnDHV1x1rxw05d7rDTQyxwOpQKdjg3ZHSyNBw5xOcd+v9yzJLdo2h4V8QXaU1HWXR8k1s8U2oofDiIipdt2+sd2cu/LA9qq/SGqLhovUVHqa1w08tVRF5jZUNc6M7mOYchpB7OPmudFq25UFhvunoYKY02oH076lzmu3sMMhe3Yd2BkuOcg9PYvLraCTqqVLEVsxiz2z3O/suLWyetQ8QjGhKFXMnuz2vDarfPHsXBydN2DXWmeFh0TYKy23Gjo46yrqKMOrZJJ2ZdKJs7mEE5AHQdvZjs0LpTSPh6C7XO0UE9Jpe43eirzPTscKlu+JsJlyPWLed0znGOir+3caNSW6logLLYKm5W2mFJR3eood9bBEBhoD9waS0HAJaT+fVQtBxAv9v0pdtIQmB1JeallVUTPa4zh7S0na4OwA4tbnIPZcz0GpknG9ujd+btpv0+F49fZG8dfpYtNq66K3FkrK/W8kv8bJ/izYKDSlPYNPRUMENZC2umqZGRNbJK11XI2LcR1IDIxjPYHotUs1Pb5NUWmnhPiKeSrp2yNlYMOzIAWkHuMLI11ri78Qb59v3uGliqBCyAMpmOawNbnrhznHJJJPXuVDW+tlttfTXGBrHSUszJmB4JaXNcCM48ui9TSUqtPTqFV/Fm/u22eZratOtqHOkvhx9kkel6ql0uzUsdANAaW5R1g7T3/pjB+ivhbITgdN4cejv1R0GAtK0jYaayWOsuFTQaHoYKq7VENLXalaal00UR2mOKHaQ1oOcvzkk49hWoS8YtTTXNt1dQ2sStvo1AGiKTb4gRhm38edmAOmc58112/ixeKS2Os9w09p+80oq5aynZcqIzeFkkO54jw8eqT1w7IXj0/D9VTp7e9r57c+nPfHJ7FTxHSVKzk+Lu2O+758W4z9CzH6U0nZ+K2po49PWuroY9KSXWOjfDvpmzFkbsxg9WtznGMEB3TC06/XK2Xvhra9feiVhoLnQajNvc2iomw09TCIRMBLEPVd16de4/NQ1Zxg1NXaguGpZ6K2eKuVodZpmtheIxCWgFzRv6P9UeePgtfOqbgdHjRXJp/Ai5G6cza7m83lcvGc427fLGc+fkumhoa8dsqjytvXtfd9V9TDU6/TzUo01h7undRt9036Ft1+lNNWe73vilHZKOTTcljjuNrpJYGugFZVDlsiLMbfUeJDt/Vw1UlTNpHmQ1cr2YaSza3OXexbvqXWdI/hbpjQNsurqs00k1dcBse1sMjnu5cI3AbsBziSMjJ6FaCurw+jUpxk6jfO1X/7Y4XPV5d+uDk8QrU6mxU7ZW6X/wCpJX46YWOjuiX03qKXTda+sitNpuBkj5RjuVEypjb1ByGv6B3Tv7CVvutrPaKX+kKyzUtqo4bebtbozSRwMbBsc2Hc3YBtwcnIx1yVommtRw6cnmml01ZrxzmhoZcoZJGx4OctDHtwfzytjv8AxeuWobvTagqNJabp7pT1sFb4ynppmyyOixta4ulOW+q0EDB6DqErU6r1CnCONrTd++22PSz+pWjUpeXdOcs3TSt2Tvn1uvobzcpNPX67cRdHP0Pp6iprDb66uoKmjomxVMckEgxmQdSDuOW9uwAA6LF0LPpao0vYqGwx8P3XVrpBdoNTUx59RIZDsbFK5haAWYAwRg/HKryLiHeor1qO+tpaIz6npKmjq2lj9jGTuDnmMbsgggYyT8QVIWfi1dLZarbaa3TOnLyLPnwM9yo3yTQNLt20Oa9oLQeoBBXE9BWjSUF/a+eu1p885t279DufiFGVd1OnxWx0ck1xxhPv26m68KLlU2HjQdKP0lardzbhVF0csDKiooyIXnlxTkbg3p5dwT7VqdDqJusOIem7dcdOafp6dl6gikZRWyKATsdOwObKGjDxgdj7T7VAU2vdSU2tRr9tUx938S6qc97AWOLgQWlv7JaS3A8uxWVX8QpKq92u/wBDpLT1qqrVVtrW+BppY2zPa9rgJAZDkZb2GO5W8dHONVVHFXcEr34kr35y73S79zKetpzpSpxk0t7awsxfTGE/sbbqjVmjZK7Wekbroux2+GkdVRWepoKAMqGVMc2GB0g6lrgHZB6DsABhddxstpj1zwvpWWmkbT3C22Z9XGIGhlQ98xDzIMYeXDoSc581W14udRe7vXXmrZGyevqZaqRsYIYHvcXENBJOMnpklbjZeMeobNbrdROsthuFRZmGO3V9bRGSppG92hjg4DDT1bkHCeTqUIRdLL6q/Xba/wBeSj1lOvUmquF0dv7k7O3S17Fl6atem6WWpgl0jYqxtfxEqrI41NAx5hpHMzsjP6mCPVx0HXAXRZLTpu/ao4b19RpOywsuT7xT1dNBRsbBMynL2xF7DkOcB+scknqqts/E/UdnpqWnjio6k0t99IRLUMe58lVs2kPIcAWHuQADnzXZauKuobRU6fqqajtzn6bkrZKQPjeQ81JJk5mHjOMnGMfHK5ZeHanLTzZrn+yS/Vr9Tqj4jp3FxaxddP77v7fwbJW1tn1hw41dWy6PsVrqtNVVF4Ke20ggeY5ZXRuZIQcydG9z1ycrfTp3h/pee06eu8+g4bUbfA6vbcKZ5ukrpIwXytmwdhyctAOB2/KhqLVtyoLDfdPQwUxptQPp31LnNdvYYZC9uw7sDJcc5B6exbJb+M+oqCmoQ6x6eq7jbaYUlHdqmh31kMbRhgDtwaS0dAS0n8+qvqPD67TjSfw3va/9sVy7/wC5P63RFHxDT/ib5rNrXt/e3wv7bfS3BtnB3UcdvrdV2K32yy1NJbrLc56esfQRunqGNeNrZJCMvjIP4T07exappXVemr1r+23PXFlsVDbYYpIzHS20RUpk2O5bpY4x6w3luenbGegWvaU1reNI3uW+ULKaqkqoZaeqhq4+ZFURSfja8AgkE4PQjss+n4kV9t1NTaosenrHapaeF0D6WkpnimqGOyHCRj3uLsg4OCOw8+q3lopRqVJKN90bXvZ3s0+nW/NvkY+ehOlCLdts72tdNXi0rX6WeL54ubZxGoJKjRbbo2zaGq44a1rPtXS5EXI3NOIpogxud2MgnthVYGUhpHPMrxUB2AzHQj25W0ag4lXG+WF2mqTT9islvlqG1M8VrpXRc+Row3eXPcSBnoBgLUF06GjUo03GorZx7Y+X0x87nJ4hWp16kZU30zjrd/Pi3OflYt7hxaY7fott7uFFoeibcK17YK/UrTUOnZGADHFDtIa0Ozl+cknHsK2RmkNJ23jZc6JthttTbn6ekuQo3RbqcSGEOJjDurRnJGMEZ6YVXWLidc7Np+LTNXp+w3qhpZnz0jbpRmY0znfi2EOb0J6kHIysmfjDqep1XPrGWitnjqi2m1vYIXiLlFmwuDQ/Idj44z5Lzq+i1dSrUlHhqS5722+vT5dD0tNrdHTpU4TV7OLeOzu/t831Muq1FpbXdiobVWaetNn1AbxDDDLaaAU7H0cnqu34OHOa4jGev+ObN1HZeG9ur7zpe61OgqK101NJT00UNO9t1gmaz7tzpiMvdu7gkg5/v83Nc5jg9ji1zTkEHBBW/wBZxq1JWQVUhsmnobrXUxpam8RUG2tlYW7XEu3bQ5w6Eho/gtdX4fVbitO3tzi9rN2s1zxZ+qvjkz0XiNKLb1EVf4c2vdJyvdY5ul0TSs+DR6dlG6KY1Mr2SNbmINGQ4+wrP03qKXTda+sitNpuBkj5RjuVEypjb1ByGv6B3Tv7CVEKZ01qOHTk800umrNeOc0NDLlDJI2PBzloY9uD+eV61RXi1a/oePSdpp3t6m962s9opf6QrLNS2qjht5u1ujNJHAxsGxzYdzdgG3BycjHXJWw3KTT1+u3EXRz9D6eoqaw2+urqCpo6JsVTHJBIMZkHUg7jlvbsAAOi0a/8XrlqG702oKjSWm6e6U9bBW+Mp6aZssjosbWuLpTlvqtBAweg6hRcXEO9RXrUd9bS0Rn1PSVNHVtLH7GMncHPMY3ZBBAxkn4grx1o9ROnBTVnGNuf9ytn7M9nzmnpzm4O6lO/H+34rr7o1+JlGaWV8sz2ztI5bAOjh+as3+jvqCWj1/a7E212uVlZNM81M1Gx9TFiB/SOU+swHb1A9p9qqpSemdRXPSd9o9RWeRjKuhk5ke9u5p6EFrh5ggkH4HyXp6yh5nTzpdWml72weVpa3l68avZr6dTftJ6rn1JxY0tFNY7JQeFvDAHW63x0zpMvH4ywet28/aVN19VZ9aWbiLS1mkbJRS6dJqaGroqURVO4TlhEkmcybh1OfMn4Y0OfiTOb/a9SW7SOnLXV2qp8WwUVNKxkz8ggSAyHIyPIjuVh0WvLvQxakihpqMjVDCys3MdmMGQv+79bp1Pnu6Lzp6KpPbKEdrSVs8Pcm/t/B6cddTjUlvlujJ5x0s1b7ouS1WTROnbPpSluEmg4aW4Wumr7m280zpK+fnDLzHIAdgAyGYIwR1VIXujsNNqm40durjJaY6qZtLOzLt0Qcdh889MdVsVo4wXy12+20c+n9PXOezs5dvrq+hMtTTNBy0NcHAENPVu4HC0qtrKm41k9wrZTLUVMrppZCBl73Elx6e0krXRaWtRqznUeHfrzm66dF39jHW6qhWoQp01lW6cWVn16v62uzItdy+xrrFcoKSkrPDvJbFWU7ZonjBHrsd0Pfz81vvF2O3zWjRV4pbLbLdPdLSaipbQUjKeN7+YRnawAdloVlubLPc4blJa6G4th3Zpq1jnwyZaR6wa5pOM5HUdQFt+oOLdVqS0RWet0PpWOOmpnUlJJDSziSlYev3ZMxAIJz1BW9enUeop1IRuovLv0s1b73MdNVpxoVKdSVtyxi+bp3+iaLfvUWmaDU1wt0PD/AEqYrfqO02yLNrZkw1cW6XcOzjn8PT1crUbDp2hs0uqq77L0bRUceoKi3UlfqMGZjWxuceTDBtcCQMEvJ7dOvVaVX8X9S3C6Vd2mobY2asudDdpGtikDRNSs2xtGX52kfiGcnyIXyi4tXmCK5UtxsFgu9JcrjJdTTXCkdLHT1EhO50WHgtB7YJPQfE58qHh2qp07d1G+eyjf7qXOM+p6b8R00pRb6OXTu52/9XFd8ehYtTonSTuL1reLZa56ObTQvz6SkaRRVVQ2N5xGw9eW4sDtvsByO61y4XC26s4OXnUc+kNP224U14pqZk9ut7IPuy0u29O3frjGRjOcZWuy8TtRXzWtk1NNcaKw1Vtght8dXS0ruVDC0uBc6Ibt3R7staMEdAFtPEDiFZazQE2lqXU9JfKyuuEdUTQWl1BT00bAc+q5rdz3OOScH4noFL0+opzpKSbfw98Wk2+lsq3LXHsTDUaepGrJNJfH2zeFly74km8J8+5u9109w2sN6k0jeKrQdLZKelFPKyWneLu15iDhKZ8Z37yD3xtOFSehbDpS519xn1VqZ9torXAaphg2c+rLXgcuIPcPXIJIxnt2UqONmpeWyodZdPvvDKXwjb26hzXBu3Zu37tu7b03bc4WsWDU8mn6O6UbLJaK4XSn8O6WupebJTjr60JyNjuvfr2HsXRpNJqqFOalJ7ml1XObtY/XL46HPqtXpa86e1JRTd8PiyxznjpxyuWbDduKNSeK1RxMsNviZJzS6ngqwXjbyeUN+1wJJb16Hv7Vr961bcr7Y7PYKuCmZT2U1Jp3RtcHu58nMfvJcQcHtgDp3yoRF6NPS0ae3avypJeyTS+zf1PNqautU33eJNtr1bT/AGQVPr0derlpUcO9PWa1QU0l68TU1V0nFNtlYN22KPmFoLm7euASAfivOKrXd/uYLg6ZPxlcVyeQXEhcVzkhERAEREBdCLIqfB7YvC792z7zd+18EHg/Bu3b/E7xt/Z2r0rlDo9i+LupfDc9ni9/K/W291wfy+a7ZnZuOPbhAcEXfWeE5o8Fv5e0Z398+a+xeC8JLzuZ4jI5eO2PigMdd9FSSV9bBQwuaJKiVsTS49AXHAz8Oq+0Xg+d+nb+VtP4O+fJcKeolo6qOrpXlkkMgkjcQDgg5BweiO9sF6Tgpp1Py3zbt1M2usNXb6Lx00sLo/EyUuGE53szk9R26Lk3Tte+xO1CHRCma/btLjvPUDcBjGMnHdY9Td7hWU3hKmo3xc51Rt2NH3ju5yBn+HZb1T0kbRDpt1wpWE2p0DqbLuZz3jmE9tvs8/auOrVqUYrdzf7Lk+n8N8N0XilaoqKaiopK7S/6ksLrlYbty7FcotqobpcLNo1s1BNyJjdHsLtoJA5TSR1HTspOrqaeh1ey4OoZHmW3NmkfBDvMT3N6y7e3Tz/NWlqXGTW3v17fIwpeBUqtOE3Ws3s3XjhKd8p7s2tm6S9TQkW7VtdUS09tu5vcdzpqS4s3TPpjFLEeh29+rcf4qdjt8LZWUEww6grDczg/qmSbH+DWrOet/DV5R/z5pHbpv6TerqOFKrwk1dLN+MxnJdJcN/lfXCqxFvtuuFPQado6+S7S26WunnkmkhpRKZHB/QEnsAPLzXCCppodTXyst1O6INtcsvLlh2YftaTlh8iev8VbzbvL4eL/AGx2MP8ATlPbSf46vPbdWV0pxck7b78LKajzhtZNKpYBU1EcBnihDzjmSkhjfiSAeimpdIvhp46uTUFnEU27lu5z8P298eouq56jnvNpipbkDNWQzl7KgtaDyi38HT49Vyuv9l7D/tVf+dqvKVRuK4u7d+jf7HLQoaGEKzS/EUYKSbvF3coRcWk2sbnw309iDRbpZqJmqbXbmzubutFRy6gk96UjcM/yloXO2Xqpq26ovVOdshZG+I7c7GhxDeh9gAUS1TV1tyuc93ZfVZNqX9PxqKnOVW0aibi9t21GDlPG5flaUOct3urWNIRbru+3H6VqrxtlkqZ5o5XuaAZWtkbtaenX2fxWXdrnQVVHd6Ka41VeY2P2Qmh2tpntPQhzewHZVeqaaW3vf5O3b9bGkP6cpzhOp+PZJJxukm701UynPGGl8O/N+hrFDpa4XCKgmhnp2i4PkjiDnOBBYCTuwOnbyyoc9+6z6W/3ajZSx01XsbROe+Actp2FwId3HXOT3UhoK96g07q233jS1uFfdIHP8PTmndNzCWOBGxvrHoT26rogqibc+On3/wCDxNXPRulBaaLUl+a/D+GPGX/u3/Jo19FnXytrrjeq+4XOAQ1lTUyzVEYj2BkjnEubtP4cEnp5LpqfB7YvC792z7zd+18FdO6TZxNWdjbrXwm1Fdqe11NNXW1rbtbqq6Qh8kgLYqc4eHYYcOJ7AZHtIWDa+Hd+vGiLlr2hfSvoLVOIKiEvdz+zCXhu3aWjeMncD36LrouImsbdFRQ0d45bLfRz0FMPDxHZBMcyM6t65PmckeRCsrhReKW18OqakuZH2betVPtFcD2MM9FsyfgHbHf8K8vU1dXpoSnh5Vl6bnj3cbL3uenpaejryhCV1h7n/wCKyvaW5+1iqNRaYr9M/Znj5qeT7Vt0Nzh5LnHbFLnaHZAw71TkDI+JUQvRctoqrPxT07p+Yb6mj0NLRuDf1nshqGnH8QtS4eAs4VVRcMB2rrY1ufMjBIHxUU/E7w3Wv+X/ANpOP2tf1LT8NtNxva271zCnGb+rdvTnJUKmp9KXGDSNNrN81MaKqrpKBkYc7miRjA4kjGNuD7c/BemKzVN8bquKmFY3l+nTrNgwRn9CkgY98OdudrnesfMnzVc1GqL/AKH4VsrNKVfgJfSyup+bHE1xZHsaQwFwOAdo/PGOyxp+K1a6jsgk249e/wAsdO5tV8Kpaecoym2luvjtu4znMX2KTRejKSitY4vXS/V9PFQ1UOkG3l0kdI2U09YY4t8zYT0c8bnHHtWqcTtRWHUugqSobfLpqK40105cd3qrR4YNiMZLqcyDo4g4eATnr7F0UvE3VqQgoYdrvOL39LdOrXsZT8LjCnKo6nF7J2V0kn1d754SeSnltNg4eXnUVmpb5RVdDHBV3mGxxsle8P58gBa4gNI2DcMnOfgVavEbixdNOcU75p27xtrtMugFPUW1sMYEpkpWkPJIyXB7gc5zgY9ipy16z1LZbfBarZcuTS01xju0UfJjdtqmABsmXNJOMD1SdvwV6Oo1OqpKcYqN1FrN8PlcLK/fkyq0NNpajhOTlbcmrWs0sPl4v+jwYF5tc9ku9dZap8b56CpkpZHRklhexxaS0kA4yD3AWt6u/s9V/wDL/wA7VsFwr6u619Tc6+Xm1NZM+eZ+0N3SPcXOOBgDJJ6AYWv6u/s9V/8AL/ztXb8SpfHzbPucE9rm9nF8exW66CSTkrvWOuEBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAXQiIvTKH32L4vvsXxAEREARF30VJJX1sFDC5okqJWxNLj0BccDPw6qG7ZZaEJVJKEVds6WuLXBwxkHPUZCzH3m5Pun2y6qPjA8Sc3aB6wGB0xjt5YXbXWGrt9F46aWF0fiZKXDCc72Zyeo7dFybp2vfYnahDohTNft2lx3nqBuAxjGTjusnUpSW5tdvr0PQhpNfSk6UIyTivxGl0UeJfK7s+Vf1MWa511RTOo5ZswundUluxo+8IwXdB7PLssgahvLa6O5NrnNqYoxC14a0eoBgNIxgj8wo1Ff8ADh2RzLW6mL3KpK+Or6cfTp2JG5agu92iZBX1e+KN25rGxtY0H24aB1XOTU99lnmqn3B3NqIPDSODGjdH7Og6fmOvxUWij8KmlbavoaPxLWOTm60ru13ud3a6XXom7drskrbqK82iE09BWmOJzt+x0bXgO9o3A4P5LqF4uQqKqr8W501ZG6Kd7gHF7Hdx17dh27eSwkU/hwu3ZXZTz2q2Rp/iy2x4W52Xsr4+QXfLW1U9LBRSy7oaYvMTdoG3ccu69zkjzXQpmu0tX29la+aemcKFkMkm1zjkSHDQMt7+1ROUItbvl+n7/cnT6fU1oTlQTcUvit2zLPp8Lf8A436HG1XiG12q507BL4quY2FpAGxsefWyc5ye3Zdmn7+2yUVyjZzW1FUyMQPa0ENc0k+tny6+wqERRKjCV0+tr/L/AOG1HxTU6eVOVN2dNSUfTde79/iefbsZ1wvl1uk8VRW1jnyQACItAZs656BoACyK3VN+uNM6jq7gXxSY3gRsaX47bi0An+KiVvOvNDaZ0bQUUUWp6qrvFVS01Y6kdQ7Imxys3Z5m85I7YwolClHanFemDJeI6xyn/wBaV5r4vifxJYznPNs+xoyktPajvOk7xT36wVxpK+m3cqUMa/G5pafVcCD0J7hUauqQncVn5j0OTaXbcK+sutfU3O4Tmaqq5XzzSEAF73ElxwOgySeyx1S6ItQkrJEtXyy6FIC/XYWL0ZFXi2is8fydjf6/Zs37sbvw9MZx8Mqh0UOupcxJTceGejLlxF1rd9RUerK6/TOu9vjZFT1TGMjcxjSSB6gAP4nZyDkEg5C7b9xO1zqZ1G6830zCgn8TTsZTxRRtlzneWMaGudnzIPn7V5uRZJUE01TWOMLHtjBr5ivZre885effuej38S9bSVouD71mobcxeQ/w0P8A5wNDOZjbj8IA2/h+C7rNxW19p+lfRWm+iKnkqJKt0TqSCRrpnkFziHsPXLRj2eWF5qRQ46dra6St7L+CfM11Lfvd+936/wAv6s9Ds13q+PVDtaMv1SL09xc6r6ZORjBbjaW46bcYx0wuWqNf6u1lHTwaivDqmGkLnQwshjhiY493bI2tbn44yvOyKy/BUlJU1dcPGPbGCHqKzUoubtLnLz79y+L/AH+7aou9Rfb7V+JrqotMsvLazdtaGj1WgAdGgdB5KPVLotI1owioxjZIyk3NuUndsvnTlZb7fqC2V92ilkoqarhmqGRNDnuja8FwaCQCSAR1IWHxn1IdX12odS7XtbcKoSxtfjc2PmNDGnGeoaGj+CpJFEqylm3dfW38EJWv62+1/wCTIJAGSsdEWBIREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQF4VPg+XD4bfv2/e7u2fgjfB+Dfv3+I3DZj8OPPKx0XpWKHbT8jnx+K3crPrbe+Fxm5XNfyc8vJ2574XH2L4gMir8HuZ4Lmbdg3b/2kh8F4abn8zn/AOrx2/isdEsDvo/Cc8eN38rBzs75XyGoko6tlXSPLHwyCSJxAOCDkHB6LpRLXJjJwalF2aMypu9wrKbwlTUb4uc6o27Gj7x3c5Az/Dst6p6SNoh0264UrCbU6B1Nl3M57xzCe232eftVdNcWuDhjIOeoyFmPvNyfdPtl1UfGB4k5u0D1gMDpjHbywuatp/xEowws/Xp+59B4V41HRSlU1Kc5ScU7v/Ym3JXve/5bLjm5O0N0uFm0a2agm5Exuj2F20EgcppI6jp2UnV1NPQ6vZcHUMjzLbmzSPgh3mJ7m9Zdvbp5/mtMmuddUUzqOWbMLp3VJbsaPvCMF3Qezy7LIGoby2ujuTa5zamKMQteGtHqAYDSMYI/MLOWlcm5Yzf72Oyh/UFOlCFJuW2DpuOE7OLluaW612muLX7rDNkra6olp7bdze47nTUlxZumfTGKWI9Dt79W4/xU7Hb4WysoJhh1BWG5nB/VMk2P8GtVe3LUF3u0TIK+r3xRu3NY2NrGg+3DQOq5yanvss81U+4O5tRB4aRwY0bo/Z0HT8x1+Kyno6kopJpc/wDHCXr0PQ0/9UaKjWlOpCU09vPovi/NOT5UbXlLh5WEbTbrhT0GnaOvku0tulrp55JpIaUSmRwf0BJ7ADy81wgqaaHU18rLdTuiDbXLLy5YdmH7Wk5YfInr/FavbdRXm0QmnoK0xxOdv2Oja8B3tG4HB/JdIvNybUVNWatxmrY3QzvcA4vY7uOvbsO3byV/KS3Sfe/3ft092cv+paP4VCNneG1vHDjFptPe18Td3aMfVt5M656jnvNpipbkDNWQzl7KgtaDyi38HT49Vi1V/u1Y2pZU1e8VbY2TfdtG4MOW9h0x8FHouuNGnHCR81W8T1leTlOo7tJPLykms98NrPRtcG3aN1Rq6xWDUtt09Z21lDdKMRXOU0jpfDxYcA7c3oz8Tursj+5apDyuazn7uXkbsd8KXsesdS6bt10tNlujqakvMPIroxGx3NZhwxlwJb0c4ZaQeqhVe3xN+xxdLHbU8jnv8Lu5WfV3d8Lf+OH9p7V/u/bv+iFXa3ziBrLSGsaKhq6W0XWmvVLSUtE+SSojNO6OJm04YG7sn81SoneLXR/s0RH83yf6p/seY10yfjK7l0vILiQuEucUREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAXQiyKnwe2Lwu/ds+83ftfBB4Pwbt2/xO8bf2dq9K5Q6PYvi7qXw3PZ4vfyv1tvdcH8vmu2Z2bjj24QHBF31nhOaPBb+XtGd/fPmvsXgvCS87meIyOXjtj4oDHXIfhP5hd1F4Pnfp2/lbT+DvnyXWzl7vWzy9wz7cIDrRd9Z4Xnu8Fv5XTG/uvv6F4L/AFnid/8Aw7UBjosik8F974zmfgPL2ftLoGMjdnGeuFIPi3as4TaioqWvq5a62uZbrRTXqUNkkyYZvwNGWdXjzHb4lajW+D536Dv5W0fj7581OVHETWNVDVU8943R1tBDbJx4eIb6aL+rZ0b0x7RgnzJXNXVd2/BaXe50ad0FJ/jpteh22Dh5edRWalvlFV0McFXeYbHGyV7w/nyAFriA0jYNwyc5+BVRcWrZUWTWE9kqpI3zUAdTSOjJLC9kr2kgkA4yOmQFZ9r1nqWy2+C1Wy5cmlprjHdoo+TG7bVMADZMuaScYHqk7fgqr4o19XddUvudfLzamsjM8z9obukfI9zjgYAySegGFlNV05ObW3p36f8AJM3QdOKgnu69uvH2+5qCIiyMQiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIC6EXn1Fv5v0K2PQfsXxUDJ/Vxf7J/zFdaeb9BY9BIvPqJ5v0Fj0EuQ/CfzC89rPsNPZ6u80dNqC5TW+2yTNbVVUMHPfDH5ubHkbiPZkItVd2t9w1ZXLzRaDxe4aWHh8zTNfpnVFTfLbqa1/acE1RQ+Fe1vMcwAs3u/Zz3Vdqq1ilwu6+jsyXG1vk/rlHoJF59RW836EWPQSLz6ieb9BY9BKvuIJ/wBNQ/8A1W/53qvkVJ6jerWJSJpFCosNxJNIoVE3AmkUKibgTSKFRNwJpFCom4E0ihUTcCaRQqJuBNIoVE3AmkUKibgTSKFRNwJpFCom4E0ihUTcCaRQqJuBNIoVE3AmkUKibgTSKFRNwJpFCom4E0ihUTcCaRQqJuBNIoVE3AmkUKibgTSKFRNwJpFCom4E0ihUTcCaRQqJuBNIoVE3AmkUKibgTSKFRNwJpFCom4E0ihUTcCaRQqJuBNIoVE3AmkUKibgTSKFRNwCIiqDsk/q4v9k/5iutc5CDHEAR0ac/3lcEYCIiAIiz7DJZIrzRyalpqyotbZmmrio5Gxzvi8wxzgQHfEgoldkN2Vy1OPH9iOEP+6Df+vIqcVj8Y+IuldcxaWtejrNc7dbdMWn7MibcJ2SzSDmOeHFzAB+tjsFXCpG95PvKT+sm0WeFFdoxX0ikwiIrkBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAWfwK0Tw/wCJF+k0Pqj7fprtcGSvt1fQSw+HphFBJI4zxvaXPb6g/C5pVYuADiGu3AHofarZ/orsdJxrs8bKp8RdTVxMTWsIqgKWVxp3FzXANkALScZAJxg4I07XmqdL6oqaWXTPDy36UZTse2aOjq55xOSRgu5rjjGCOmO6ieJq3VL9Wr/b3vyI/lafR/5/nHbqjVkWdcrFe7LHRzXizV1DHcIG1VI6pp3xCohd2kjLgN7D5OGQsFSAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgN44Z8UpOF0tTdrNpOz1t+dkUN0rhK99AHRuY8xxteGFxDuhcDjHZaQ5xe4uccknJKmdE6ej1brKxaWmqzSsvFypqB04bu5QlkawuxkZxuzjKm+Llo0Jp3WFZpnQtJfoorPPPQVj7vPFJJNPHK5pezlMaGtIHY5KiWGpPrj6W/lfUR4aXT9/wDH7HHipxHdxLvVtuEVnFqo7RaKSzUdGJ+dy4YGYzu2tzlxce3TOOuMrTEROrffP1yOiXay+mAiIpAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAf/2Q==)\n",
        "\n",
        "- `clb0lbwzddp1s086uh1e2gw0y`  \n",
        "![2.jpg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCADaAlQDASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAYHAwQFAQIICf/EAFAQAAEDAwICBAkJBQUECQUAAAEAAgMEBREGEgchExUXMRQiQVJUkZLR0jJRYWNloqTh4iMzcoGTCBZCcaEkNGKyJTZzdoKDs8HCN0NEsbT/xAAaAQEAAwEBAQAAAAAAAAAAAAAAAQIDBAUG/8QAOhEAAgIBAgQDBgQDCAMBAAAAAAECEQMSIQQxQVEFE2EUInGBobEykcHwQoLRBhUjYnKi4fEzUsLS/9oADAMBAAIRAxEAPwD+f1R+/k/iKxrJUfv5P4isa62ZBERAFdv9lDVt3tvEeDSJvTIrFfIqltdbJzH0NykFNKIoCHjBc55a0DIyTjyqkld39kmwX2p4p02pae01DrLbaatjuN027YbeH0kobI6U4DHA4wc578I60Tvlpf2/r32sh81XO1+/+tyrtWaG1noiohh1jpW52SSrDnwMraV8PSNB5lu4cwMju+dcJbt1vF4u8rXXe71lwdDlrH1E75cDPPBcTgFSjijw3bwzqrBbpr0a6tu1jpLxVQ+DdF4E6cEiAne7eQBkuw3vHJVVpJvvX3f2ReW8ml8fsvuyFIiKxUIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAyVH7+T+IrGslR+/k/iKxowEREAWVlZVx0z6NlVM2nlcHviDyGOcO4lvcSMlYlY/A/QGp9Ta30/fqTSFZdLBbr3R9aVPgxfSQxNlY+QTPI2NaGZJ3csd6tCKlJJukVk9KtFdOjkYGuexzQ8ZaSMZHzhbV2vV4v9abjfbtWXGrcxkZnq53TSFrWhrW7nEnAAAA8gACmfHLXV61zxHvdRctTOvdFQXCrpbXMHNMbaQTPMYjLQBtxjBHeoAsoS1xUmuZpOOiTigiIrlQiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIDJUfv5P4isa6E1u3Svd02Mkn5P5r46t+u+7+anSwaSLd6t+u+7+adW/Xfd/NNLBpKc8F9RXKy8S9KRRahqrbQPv1C6q2VTootnTMDi8AgEbc5z5FE+rfrvu/mvTbMAHp+/wD4fzV8dwkpVyKyjqi0THjrpXUOl+KOo+vdOVFojuN1rKuhZJD0bJad079j48ci3GOYJCgC6c8NZVMijqrjNM2BnRxNkJcI25ztaCeQyTyCw9W/Xfd/NY48bxwUOxpOWuTl3NJFu9W/Xfd/NOrfrvu/mr6WVNJFu9W/Xfd/NOrfrvu/mmlg0kW71b9d93806t+u+7+aaWDSRbvVv133fzTq3677v5ppYNJFu9W/Xfd/NOrfrvu/mmlg0kW71b9d9381661OY4sfIWuHeCzB/wD2lMV1NFFu9W/Xfd/NOrfrvu/mmlg0kW71b9d93806t+u+7+aaWDSRbvVv133fzTq3677v5ppYNJFNLNw463tsNw656Lpd3ieD7sYcR37h8y3eyb7f/C/rWiwZGrSIsr5FYPZN9v8A4X9adk32/wDhf1qfZ8nYWV8isHsm+3/wv607Jvt/8L+tPZ8nYWV8isHsm+3/AML+tOyb7f8Awv609nydhZXyKweyb7f/AAv607Jvt/8AC/rT2fJ2FlfIrB7Jvt/8L+tOyb7f/C/rT2fJ2FlfIrB7Jvt/8L+tOyb7f/C/rT2fJ2FlfIrB7Jvt/wDC/rXp4SODQ8307SSAfBORI7/8f0hPZ8nYWV6isHsm+3/wv607Jvt/8L+tPZ8nYWV8isHsm+3/AML+tenhG9rWvdfSGu+STScj/l46eRk7Ar1FYPZN9v8A4X9adk32/wDhf1p7Pk7CyvkVg9k32/8Ahf1p2Tfb/wCF/Wns+TsLK+RWD2Tfb/4X9adk32/+F/Wns+TsLK+RWD2Tfb/4X9adk32/+F/Wns+TsLK+RWD2Tfb/AOF/WnZN9v8A4X9aez5Owsr5FYPZN9v/AIX9adk32/8Ahf1p7Pk7CyvkVg9k32/+F/WnZN9v/hf1p7Pk7CyvkVg9k32/+F/WnZN9v/hf1p7Pk7CyvkVg9k32/wDhf1p2Tfb/AOF/Wns+TsLK+RWD2Tfb/wCF/WnZN9v/AIX9aez5Owsr5FYPZN9v/hf1p2Tfb/4X9aez5Owsr5FYPZN9v/hf1rj6h0P1D4P/ANKdP0+//wCxtxtx/wAR+dRLDOKtoWRZF0HWktGenz/4fzXx1b9d9381npZJpIt3q3677v5p1b9d93800sGki3erfrvu/miaWDff8t3+a+V9P+W7/NfK0AREQBfTvkt/y/8AdfK+nfJb/l/7oD5REQBERAdXS09spdS2upvTY3UEVXE+qEkfSNMQcN2W4O4Yzywcrs6kuGlKrTcNPaY6ZtwF2q5XmOlMbvBnH9mN20eL8zc8vmCiK9HI5VXBSkm+n/H9Du4fxCfD4J8PGMWpXu1ur7blwCKwf3/bwybpS0dWNi8GdUGnHhhd0G/pOm+UHbv5YVdV2mhR6dZqAVu8PuM1v6Ho8Y6NrXb92fLuxjHk71ZNon01fNXVPEm1Xiaaqo6J1dUWYUzun3iLoyGvOGuaCQcjJx5FELXcNM33Sb9PX6+mzVMN0kuEcxpXzxyNkY1rm4Zkgjbnny+lceNyi+t1G/je/wC0fY+JYOH4mFTcPell8p6opaEoaFaaSrf3ZO07tW9/JOGkrdSssIvDBAy2sulTVvgIEMJaHO8QElxGQBz558i07/ount1ogv8AZLy+5UMlSaSQy0b6aSKXGQC1xOWkf4s/QpRBxGssevqq5U9ZVUlvmtQtEVc2PMsWGt2z7P8ANvd349S5WtNTQ19pprdLry46kkNT0sw6AwQMjA5AB7dxfk9/cB5CrRnmco3y2+79Ntvgc3FcJ4JHhs7xU5JySalypR00nNOSbvdQnavdUj4u/D/T9nqKy0VGuoWXiiiL3081E6KFzwMljZi7mfm8UZW9o/Tuia3R19ra68vdUR09O6V5tm51C4vPyDv8fPccbe5dZusdO09JVxVnEKrvlpko3sgtNdQPfUbyzDQ6VzdoLXc8g+RRHQ92scFp1Bp+93Pq5t3p4mxVLoXysa5jycOawF3PPeB5FW8s4STvp09d+n9fizfR4VwnHY3ijj0yWRU5Wl7vu21lkt26t6er0xdM0KOi0xDfiwasqYaOBrZYK0Wwue6UEENMW/l5eeT3d3Nb+vbde6riFU2auunWtxkmgp21BgbB0rntbt8RvJvygP5LnwWTTUl0npJdbUsVLExro6s0VQWyuPe0NDdwx85Ck+sbtpZ+tG67sup4bg5lbSzihbSTxvLYwwHxntDf8H+q2v34vd7Pp8PTbrZ5McEZ8DkhLRD/ABIuo5VvFKer3Xkkpafd0unJ3s3bNGu4dWtjbpR2fV8dwutnhfPV0gonxt2sIEgjkJIcWk92BleW7QNjmtdkuF41kLe++h4p4fAHS7XNkLPGIcMNyBz+nu5ZXVkveirLXah1PadSSV9TeqaoigoDRSRuhfO4OJkefFIbz+STlcKvv1pmtei6aKr3SWkSeGN6Nw6LNQXjyeN4vPllUg8sqTb3avbltK+nK0t9/idnEYPCsEpTcINpSqKm3F/4mNRdqd6nBzbVrZW4o+7NSWjSGpb7aNXClkdTUVRTQmSDpmmo5bC0bTg4zg8sfOuZq6rsVWbP1EyBohtNNFV9FB0eaoA9IXchud3Zdzz86919dKC96yu11tk/TUtTUF8Um1zdzcDnhwBH8wo+tMUXNRyT50r/ACf9fseP4jxkMLy+H8OovEpy0y5trUq966e0VT7Nkx1vorTml7Fpm62TiBbtQVN8ojU1tHTRhr7ZJhn7KTx3EnLnDmGnxDy5qHKY63tXDO32LTNRoXU9xul0q6IyX6CppzGyjqcM8SMljdwyXjkX/JBzzwoctur+L+/27PqeJ0XwRfPBe4aXodEXyPUMdM6oqrRPDbOmpulIqTNyLDtOx23d43L/ADVrWAaF1Txy09T2G2W6qs8lE2Oan8ADIXzNpX790bmgOO8ZzjmRnK/P2jf+rdH/AOZ/6jlZfCK/2nS/ES0X2+VfgtDSumMsvRuft3QvaOTQSebgOQTieFvFPLBvVodJf6e3c7eE4nRKOKSWnUrb+K69tiSM0DdLBw313ctTaSkopmzW7q6erpNr2A1DhJ0TnDLcgtBx5MZWGi4OWuSe2WC6a8goNT3elZU09sdb5HxtMjd0cb59wDXOHk2nB5c+WdCg1vDNoLW1jvF9qp6u5y0DrdFM6WQObHO50mCQQzDdp5kZ8mVLItXcOLtqmwcTbrq2eirbTSUwqLM23SvkkngbgBko/Z7XEA5cQQO/6OWUuLxa7tW+cY3/AARqk09ru33XNHoSjwmWa0JOlylJJfjldtOPTl6dyN6Y4M1Wop7FC+9ilF0luEFY402/wGSmIG0+ON+7c3zcZ8vljep9Gy6YtNor6mt3z3R1WHU5i29CIJjEDuyd24tce4Yx5VONO8UrTbNC6jD6kw36ouklXboBG4+JO+EyYeBtGBF5SM5XF42ap07qjVMEuk6w1FrpqXbG7onx/tJJZJZOTwD8qTHzclpgy8dLiVDIvdt3ttyb5/zJL/S/Uyz4eAjwspY37/Tf/Mly+Cbf+pdKqGUVnqKi70VpqM07qyaKIOI3bQ9wbuxnnjPdnyK1ZeAdjiuTbYeJLekddH2XnaJP992h7GfLxtLTzdnAPLn3qrbHXMpr/ba+und0VPVwySPdl21jXgn6eQHcFclVxL0TJqaO4MvWaduum3kv8Gm/3MQNZ0mNuflAjb8r6Fp4hPiozisDaVO6Se9quafdmHAx4bypyzpOSaq210k3ya6pL5+pXtg0Np6rp6ubU2t4rTLT1bqNlLTUL62oeW/KkLGuaWx+QOPec8u7PWpuDErtbXbSNfqWClhttrfd2V/gznslgAYWksyHNyH8+8jBGCpJYOIdgg0u+2WfiFLo6uZdquqqpo7W+oNxjkfmNwLRkFrfF2uwsl44laMrOIOoL7FfZJaOv0i+2QzyU0odJVFjBsI25BJB5/J+lckuJ45zlpTWz6cntVe713/il6pHow4Xw6oqUl+Jb6umqmn73RdaXdNkHvvD6wW+y2zVVl1sLpZKy49W1VQbe6CWklxuP7IvO8bMu7xnkPKs54R1MGurvpOvvTIKKzUctxnuQg3NNK1gcx4ZuHytzBjdyye/C55v9oPCBulvC/8ApQakNeYOjd+48FDN+7G35XLGc/RhTjVGqxFwPsks9N0V7v8AA21STZ8aW30cr9p+fmS1p+fBW+XJxeKoKTdycVaV701Lkk9KT6U/kcmHHwmZuUopVFSdN9G0483Wrbrs+TVlNw0s9SX+Dxl+wbnY8gUx4T6Dqdcaoo4zFTS26krKbw9ktSyJzonP5hoJBdkNcMN5+sKFskkjz0b3N3DBwcZHzLu6AulDZNb2G8XOfoaSiuEE88m0u2Ma8EnDQSeQ7gMr0+IWR4peW6dbdTysLgsieRWvy/qTvi7Z7tRVbIbtYtKWazG6uigntFNT+Etiy4De2N292GcyDjmB3FaOu7dLRcMtOvteu5b5p7w6oio6eW0tpHQSDJe7duc92S48iVDr5Pbr1rW4VLbg2Cgr7pNIKt0byGQvlJ6QsA3EBpzjGfJjKsS6jhrWcOrToyLilR9Pa6ypqzL1TWbZekAw0Do+R+krytE+Hx4VK3TTaUG+nN7Saa9Gnz2PZ14+JzZnGkqaTc0vglvFNP1TXqcq18KtNTWnTdxv3ENtrm1O13glP1W+ba8SGPDnNeAG52+MfO7sAladt4VOF11PS6o1FBaKDSjmsrqxsDqgue95bG1jAWk7iD5RhZbpqqwVNt4c08NfuksLXC4Donjoc1O/yjxvF5+Ln1qQVes9DX+/cRbRcNQPoLZqqanqKK5iklkY10Em8B0YG/Ds47uWFMsvGRbdunq/hWyWRJVtv7ltXd1aM8WPg5wjqStKH8T3bxtu99vfpOqq6ZzNd6b0lbNKcP20N0p30dc64eE3inoNsssYqGgOfGSHOcwEt2l3LBAOOa1uJlNV02kNHOp9bS3+ySxVbLa2S2Mo3UzYntY4cnOc7J848tv0rJq+s0LcrDojR9p1kJKe0eHx11fLQTMbF0soeHdHguc088AZOMZAPJbuqGcOrjoXT2n6PidSSVOnIq446qrAKl00vSNa3LAG92Mk45quOU4eVKep+/Nu4N7Nzp/htPlyrZ8qLzhGbyxhpVxglU0t6ha/FTXO7vdc7MNJwd047+7lLc+I7KG4anoqeqoaY2t8g3y8mse8Pw0bsNDvLz5DHPi2PhrDPHqGu1XqJlkt+nKoUNROyldVOlqS9zRGxgLc/JJJzy5fy7Fz1npqo1Nw0uENy3U+n6K2Q3F/QyDoHxSh0gxty7A83OfJlbVs1NpzUcut9J3Bt3lt18u7rtRVdso+nfGWyvIL4zh21zXN+bB+ZS8vGQg5Nvf0Wy11tt/67733QWLg5TjCKXTq928d778tdJ1VcjnVfBsHUWlbHZNVQXCLVUDqqGqdSuhbFE0uJO0kuJ2tJxy58vpXI1Ho/R1vtNVcNO8Q4rpU0U7YZqKot76OVwJI3Rhz3bwCOY5EDmVPuIL9PWW78N6atrr7aqCitI3TRYiuFM0vd0chbg7XZAcWjJxkDJwtLWut7HX6FudlunET++lwnngNscbU+nfRBriXvdK9gLtzfFxk/wCqph4nip+XJNtN9v8AO1u1Fr8NdY97Zpl4XhcbyQaSaSfPq8ae1yT/ABctpdqKhNNOKcVRjPRE7Q7yZUl0jpPT98oaq46g1lFZ2QyNiip4qN1XVTOIyXCJrmkMA/xE4zyUXMkhjERkdsByG55A/PhWvw31tZ7PoepsVPrh+jrwbl4VLWtt76nwyn6MNEeWAlu05ODgc/pOPT42eXHhvFd2uXx+EvszyeBhinmSzct+e3T4r7o5cXCCSTiNa9DDUUZpbzR+H0dxFK4boDC+RrnROcC0noyCCeX+i07zw9scWlKzVOldbRXxlrqIqe4QmhfTGLpCQx7C5x6RpcCM4Hz4U4quJWkJuMGldVy6lmq6C3WMUlbXTUsjZDUdDO0lzA0nJc9uduR43fgKk46moihkp455GxTFpkja4hr9vMZHcceRcvDPi8zi5ycaUW9lu9UtV2k70pcqq066HXxK4TCmoRTty6vb3Y1VNr8TfO7pq+paI4HU5rBpk6yaNUmm6fq7qyXoBJ0fSdF4Tnbu2/8ADjPLKq+GkqZ3vjihc58YJcO4jCv668X7Vdqt+o4+Lt7ttJLSh7tP0lvPTsnEYb0bJnNMW0uG7cc8iRgeT8/9PMJHSiVwe/O4g4Jz3qfDcnFZNXtPZdKp72vwq1y/9viyPEsfCY4x9m9et2qVX7zp/KPw2JNw80RFru519BNemWuOgt01xfO+Ayt2xluQQCCB42cjJ5dxWxqjQtntemqXV2mNWtvltmrXW+YvoXUskM4ZvA2uc7cC3nnK7fAZlNJetSsrZXxU7tMVwlkY3c5jCY8uA8pAyceVaeqLjo+0cPaXROnNTOv08l4ddZ6llFJTxxNEPRNjAkAc4nvyOXkSebL7Z5cW6Wnatqd3brb03XzJx4MPsXmySt6t26eyjVK993vs/kdmv4IWOn1BXaNouI8NRqKmp3Tw0TrY+NkuIuk2GXeQx23Jxg8sc/IOFp/hraqzTdDqbVOrX2aC6zSRUMcNskrHyNjdtfI/Y4bGh3Lyk/MtfitqihvvEq7al0xcpH01QYugqIw+JxAgYx3JwDhzDh3f6KY6N4j0cegrPpqLiXV6Nq7PNUNmLbfJUsrIpZC8OGwHDmkuGDgHPesHLjocNDJqbctN7K47O+UX1pfhbW/y1guBnxEsbiko6q3dS3Vc5L1f4knt841ScIapmrb/AKbv18it9Np2kdXVdbHTvn3Q+LtdHGCC7Ie094xz+Zcy76Ks3hFnh0ZrGmv5vE4pGxOpzSzwzFwa0PjLnENJIw7OO9SK36usVXxEvF9l4g6ltXSwdHbr09ofI5zQ0AVEcTMujdt5AAYw3OV0NY8RLC6fSFw6/bqu9WS5Cuq7nHbzR74Gva5sGHNaXnxSckcsq0c3GLJBSt2le2107v3e/wDmTXZiWHgnjyONKm633q1Ve98aemV90cy68HbfBT3mCx6z6zu1ggfUVtG61ywRlkZxL0UxJDy0+TAz5F8WvhVpqa06buN+4httc2p2u8Ep+q3zbXiQx4c5rwA3O3xj53dgEqVav4mW6tor9V0vGS9V8FxhlFDZYbeYXRGT/BNI9u0xsBIIackdxHlh101VYKm28OaeGv3SWFrhcB0Tx0Oanf5R43i8/Fz61lgycdkhFTbVvd6d17sm+cUq1JdH2tl+JhwOKbeOKaUXSvZvXBLlJu9Llta23rY7fDDh9pql4of3X1lc2TXCgrJ6YW00HT09W1sTjvMhcA3zgC0/JHz8oRqSi0pYrjT1Wl9RuvT2VDnywVVq6COPa4ENIL3CRp5gjkMD6VLIdfabouP0muxVSTWZ9wlf08cTgejkjLN+xwDuW7JGM4Hcodq+06Zt07Z9PazgvoqJJHPEdFPTmEZyM9K0Zzk92e5b8P50s8Z5nL3oR2ra97T22r1afS3yKcT5EcOTHhUXpnKne+no1vv8k11rqT3UVtqbpwruGpdXaFs+nquGopDZqiiom0T61sh/aNMbfltDMODsfz71raPv/C+jpNDx36C2ukopLmb10ttMpcHteKfpDsPS8y3GN23l3YUW4jXGkuNyt5otaSakip7dDTid1vNJ0GwYEQYQM4AHjeXPecZUSV8PB+dgcZtpNtpK1Vx0172/r0335GOXjXgzRlBJtRSd07d6r93b0+C39PTzOVDuIP8A+B/5v/wVnaG0rLrbVlu0vDVCmNfIWGYx7+jaGlznbcjOA08shVzxRggpa+GlpanwiGGaojjm27ekaHNAdjJxkc8ZK78zWlx6/wDJ5a3ILJ8grCs0nyCsK4iwREQBERAWE7h/Zi4nwmt9tnwrzs+svpVb7bPhUviop5oJaqNoMcPyjlYoo3zSNiYMuecBd/lw7FLIr2fWX0qt9tnwp2fWX0qt9tnwqV1EElNM+CUAPYcHByvuopJ6Vsb5mgCVu5uDnknlw7CyI9n1l9KrfbZ8K9PD+zEAeE1vL/jZ8KlopJ3UrqwNHRNdtJzzyvmmppauZsEIBe7uycJ5cOwsifZ9ZfSq322fCnZ9ZfSq322fCpS+NzJHREeM1xaf81kqqSajkEU4AcWh2Ac8inlw7CyJdn1l9KrfbZ8Kdn1l9KrfbZ8Kl0dHPLTSVbGgxxHDufNKSkmrZeggAL8E4Jwnlw7CyI9n1l9KrfbZ8Kdn1l9KrfbZ8KmdqqmW67UlZO1xZTVEcjw3BJDXAnH08l0rxfqS4WnwGKOYSeHzVWXgY2vJwO/v5rOUVGSSjaO/Bw2HLgnlnl0yXKNc+XW/3RAaLRlHbahtXbrtdKWdoIEsM4Y8AjBAIbnmCQsJ4f2dxLnVdcSeZJkZz+6rgp4YzpZumw3M89vfc+XeXbwWD+bR/ouHbrZYhYRebu+uJNW6mDKYsGQGB2fGH0n/AEWUcmN23Hk6+Pb62eln8EzY/LhHImnDW72UXScl1tpONvryrYrrs+svpVb7bPhTs+svpVb7bPhVpN0jRHUvVQq53UrqbwuPaB0z24yGDybvcsFTZLMyooImxXejdUVTYZYayMNd0ZI8djtoHL5iFKy4W0kue5lPwDjMcZSnSqTjz5tNLbp1Xx6WVp2fWX0qt9tnwp2fWX0qt9tnwq14tE00sdPtmn3uuDqeYbm4bAHvbuHL5X7M/R9Cx23SlDU25lzfT3WrjqJpGwto2tyyNrsbnlw7z83JR5/D1Zqv7M+IOSi4rdXz6Kt+/wDElt1ZVnZ9ZfSq322fCnZ9ZfSq322fCrPZpOip79cbZcKqcU9HSPq2yMADi0BpGQc+QnI5dy1aqG2UdDT6g07WVcUsdSYXR1BYXtdt3Bw2ju/krLJilWlc6+vI55+CcTgjKWZqOltNWtXutKTStXV99+hXXZ9ZfSq322fCnZ9ZfSq322fCrXuep7/Hp201LLnKJKk1Ildy8cNcAM8vmJXMOn4quhtNXa3yPdWzGlnDyD0c2foHIEc+fkUQlGrnFLdrvyv0XYtxHhKU/L4Sbm1GMmnFRdTUWqqUrfvK+VeqK77PrL6VW+2z4U7PrL6VW+2z4VaEOn9Pvqb251TWmktWza5jml7+ZDu8Y7wcf+6wVGnqCrls7rLPUCG7PfHip2l8Za4Bx8XAI55/kiyYW6r91f2In4FxcYalpfomrdT8u16aqXzsrbs+svpVb7bPhTs+svpVb7bPhVsXHRtJFQ101LT3WCShYZOkq2t6KZoPPbgZB8oytez6poLfT2eGaKoJt8s8ku1rSHB4cBtyeff5cKPMhOOrFG/+rNn4FLhc6w8fkWNNWnVp+8ovtyTb9UtudkNttvgtdFHQU7nujizgvIJ5knyAfOtldnSFPpes1HSU+tLjV0VneXeE1FK3dIzxTtwNrv8AFjPinl5Fo3aO2xXWsis08s9AyeRtLLK3D3xBx2OcOWCRjPILr5NRPnzURZ6iknpWxvmaAJW7m4OeStLT3FrT1potPU1TR3JxtNiuNsm2RsIM1Q7LHNy8ZaBjJOD8wKw4nNkwxUscNT3+zf3SXzOjhsWPNk05J6Vtvz/iS+zb+RUyK9OFNJR3vg5ddI1TW9Jf7zNSUjndzKplI2aI/wA3Rbf/ABLSvOlaLVWpdIWW7SVMMcehYqg9CQ14fFFM8A7gRjLcHl6lyPxOMcs8c41pv8krv6NUda8MlPFDJCV66VerkopfVO/j2KYRTvSmhLRfdFT6jq6msZUxXyjtgZG9oj6KUjccFpO4eQ5x9BU9n4OcL4rwy1i46py6+v06Tvpz/tBjbIyTOz5ABwRjJPMY7jpm8TwYZuErtXyXZJ/qimLw3NlxrKqSavd+sl/8sodFcsVHpC38IKSk1m+7S0tLqashjbbDG2V7xG0ZJkBAbgE4xknHdzWjS8HLbV8RqnTlLcbhVWeC1C+MdBC11ZNTOY1zYmt+T0hLwPm8uPIoj4ni97Wmkr36NKvrvyJfhmV6VGm30vdW2vytcyqFs1dzuVwjp4a+4VNTHRxiGnbNK54hjHcxgJ8Vv0DkrD4hcLqLTulqXVtqt+orZE6tNDUUV8jjEwcWlzZGOYGgsOCO7OQu1d+GXCmk1jcOHtPe9QwXeKndJT1dU+DwRsghEojf4occj/EMcyBjlkx/eXDuKmrf4nyutNX90T/dnExlodK6W7q9V19U/wAvmUyisPRnESzad0tbrHW01c+ak1VS3yR0TGFhgia0OaMuB3+KcDGPpCiGqbpBfNTXe9UrZGw19dPVRtkADwx8jnAOAJGcEdxK6seXJLI4ShSXXvy/fyOXJixwxRnGdt81XLn/AEX5kcvtfNbLVPXU7WOki24DwSObgPIR86iX9/Lx6NR+w74lJNXf9Xqv/wAv/naq3Vc05RlSZgjvniDec8qajx/A74k7Qb16LRew/wCJRhFj5s+5NEn7Qb16LRew/wCJO0G9ei0XsP8AiUYRT5s+4ok/aDevRaL2H/Et60cX9a6fnfVWGvNtnkZ0b5KSWaFzm5B2kteCRkA4+hQpFVzlJU+RKbi7RMLjxU1Xd6t9fdpo62qkxvmqHSSSOxyGXOeSVrdoN69FovYf8SjCKVOUVSYbcnb5kn7Qb16LRew/4k7Qb16LRew/4lGEU+bPuRRJ+0G9ei0XsP8AiTtBvXotF7D/AIlGETzZ9xRJ+0G9ei0XsP8AiTtBvXotF7D/AIlGETzZ9xRMKHipqu2OlfbZo6R1RE6CUwOkjMkTvlMdh4y04GQeRWt2g3r0Wi9h/wASjCKNck7sm3VEn7Qb16LRew/4k7Qb16LRew/4lGEU+bPuRRJ+0G9ei0XsP+JO0G9ei0XsP+JRhE82fcUSftBvXotF7D/iTtBvXotF7D/iUYRPNn3FEn7Qb16LRew/4k7Qb16LRew/4lGETzZ9xRJ+0G9ei0XsP+JO0G9ei0XsP+JRhE82fcUTCg4p6qtVS2ttcsVHUNDmtmp3SRvAIwQHNeDzBIP0FcS86jrr30PhcUDOh3bejaRnOM5yT8y5SKrnJ82D6c9zhgr5RFUkIiIAiIgLq3OALQ4gHvGe9eAkHIOCEPevF6ZRHpJcSXEknvJQuc7G5xOBgZPcF4iA93O27Nx25zjPLKAlpDmkgjuIXi9+ZACSTknJKOc5xy5xOBjmV4iA93ODS0OO094zyRrnMOWuIP0FeIgC9bt3DcSG55kDJwvEQEt/vzMy/sqIZaoWhm1gpSQP2YZtI25xny965FRc6J1hNop45gRcH1LC4DHRlgaAefyuX5rkosI8PjhVLt9D18/jnGcSpLLK9Wrn01VaXZbKlyVEkrL9Za+8wVlXR1T6ZlGynO12yRj2j5bcHBx9JXl11DRSWmG22+a41D46kVHTVrmlzMDAazBPJRxFC4aCa9CZeOcVOORbXO7db711+W13XSibya7t/WFZUw0lR0UtIGRNIaC2YFxyefdl7uff9C5duv1tNop7Xc5LlAaR73RyUTwN7XHJDgSPL3Hmo4irHhMUVS9Pp/2b5P7S8fmnryNP8Sqtqk02vzSa7Ud6G/UMVwutS2CobDWUUtLAwyGRzS4AAuc45xyJPfjPJcFEW0McYcv3R5XE8Zl4pJZK2beyr8Tt8vouh0q25QVNmtlujZIJKMzmQkDad7gRjn9C7mkLlLa7Jd62RgMMAY6AuHdUOBaMfyPP+SiKKmTBGeN4+jd/W2dnB+L5uE4tcYt5RjpXTlDRF9brZ+tdCU6SdTGz381wldCYYuk6MjfjceYzyz/msFRqGgpJbO2ywVBhtT3SZqNofIXOBcPFyAOWP5qOoo9ni5ucuvT5UXXjebHwuPh8KUXFVq5t+/5i+G9d+XrRJblfLHPT1bqV13lnqs7WVE22KHJycBrvG+YAjCjS7Nx0Zq2z2yO9XXTdxpKCbb0dTNTuZG7cMtw4jHMdyqr+/l49Go/Yd8SQ8vAqTOTjuOy+ITWTKkvgq5u79f2kWrpCp0vSaio6jWdvq66zsLjUwUr9sjxtO3B3N/xYz4w5eVaN2ktst1rJbNBLBQPnkdSxSuy9kRcdjXHnkgYzzKrV/EC8h2BTUfL/AIHfEvntBvXotF7D/iU+dC7OKiwi5zsbnE4GBk9wXir7tBvXotF7D/iTtBvXotF7D/iU+fAUXnaddRWXRFHZKFk7LtQ6kjvcUu0dEGsh2AZznduHdjGPL5FLtQcZdOVvFC2a0ttkrTa4LWbbVUk2yOQsf0ok2FrnDkJeWcZxg471+Xu0G9ei0XsP+JO0G9ei0XsP+JcWThuFyyc5J27/ANypnZj43PjioxfLT/tdr6n6Nm15oaxWGm0xoykvklJJeYLrXT3JsIk2xEbY42xuI7ueSRz/AM+XWqOMWmZr+y6toboIm6wbqAtMUe7wcQtZt+XjfkHlnGPKvy52g3r0Wi9h/wASdoN69FovYf8AEqPg+FbuVt773zuv6Iu/Ec7joVJdkv8AV/8Apn6Ph1tw7u+knaW1VT6hjAvdVdY5reyElrZAAGkPfgnGc/NywTzCzR8XbO3iBWXttprobFWWjqHo4Zg2ripQxrWyNd8kSAsB78eTPlX5q7Qb16LRew/4k7Qb16LRew/4lPsnDb3dO9r71f2H9451TVXd3W/O6+FsvfWmpdJ19opbTpuXU1bK2oM89beqvLtuMNjZEx7mAeXcfGz9HJaXE7VNv1prm56mtcNRFS1piMbKhrWyDbExhyGkjvafL3Kle0G9ei0XsP8AiTtBvXotF7D/AIltihhxNSV2r+tX9kY5uKyZk4yqnX0uvuywV0dOWh9/1BbLGwuBuFXDTZHeN7w3P+qq3tBvXotF7D/iTtBvXotF7D/iXSs8EzlcW1RY3Ge1WexV2obLYZ6iahoKoU0UlQ9rpHbJGtcSWgA+MD3AcsKml1rjrK6XOjkoaiClbHLjJY1wPIg+Vx+ZcQyOIxyXLN3W90i73baPhERUAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQF3xUc88MtTG0bIebiSsUcbpZGxsHjOOAm5wyA4gHvAPevASDkHBC9IojJUU8lLM6nmAD2HBwcr6npJqaOKSUANmbubg+RYSS4lziST3koXOdgOcTgYGT3IDM2kmfSvrGgdGxwaTnnkr4ggkqZmQRAF7zgZOF8bnbdu47Sc4zyQEtILSQR3EID6ljfDI6J4w5hwVkqqOejc1s7QC9ocMHPJYSSTknJKOc52NzicDAyfIgM0VFPNTy1MbQY4flHK8paSatmEEABeQTzOFiDnAFocQD3jPejXOactcQfnBQG3a6lltu9JV1DXFlLUMkeG4JIa4E4+nkuleL9SXC0+AxRzCTw+aqy8DG15OB39/NcHvXrdu4biQ3PMgZOFnLFGclJ80d2DxHPw+CfDY60z5/Ov6E5p4YzpZumw3M89vfc+XeXbwWD+bR/ouHbrZYhYRebu+uJNW6mDKYsGQGB2fGH0n/Rb/8AfmZl/ZUQy1QtDNrBSkgfswzaRtzjPl71yKi50TrCbRTxzAi4PqWFwGOjLA0A8/lcvzXJjhlV2q1NP8+f0o+n47jPDsmmUGpvFGWNJrZ6VHQ1u7Tep269UdVukaI6l6qFXO6ldTeFx7QOme3GQweTd7lgqbJZmVFBE2K70bqiqbDLDWRhrujJHjsdtA5fMQlZfrLX3mCsq6OqfTMo2U52u2SMe0fLbg4OPpK8uuoaKS0w223zXGofHUio6atc0uZgYDWYJ5JHz243f7v9/YrlfhMceZ41DaTa5tte60knTS5pNNrnqWxuxaJppY6fbNPvdcHU8w3Nw2APe3cOXyv2Z+j6FjtulKGptzLm+nutXHUTSNhbRtblkbXY3PLh3n5uS25Nd2/rCsqYaSo6KWkDImkNBbMC45PPuy93Pv8AoXLt1+tptFPa7nJcoDSPe6OSieBva45IcCR5e481SPtLTcr6flvf6HZk/uCGWMcWl+7L0WpOKTtpreOp73vRlZpOip79cbZcKqcU9HSPq2yMADi0BpGQc+QnI5dy0bnb7J1TFdrNPVAGc08kNSWF4O3cHDbjkskN+oYrhdalsFQ2GsopaWBhkMjmlwABc5xzjkSe/GeS4K6MUcrac29kv+TweO4ngMeOWPh8cWpOe+9xVrTTdOq5Wk2uZKbxqqguEV1ZDFUtNdFSxxbmtG0xuy7ODyB8mMrFpii0LU2e+zaqu9wpLlBTB1oip490c82HZbIdpwM7fK3kTzUbUk0xW6FprPfYdVWi4Vdynpg20S08m2OCbDsukG4ZGdvkdyB5LSOKOHG1C/16Lb8jzeO8Rz+JZY5eIatfJbycn9ZP5EeiifNK2KMZc84C9qIJKaZ8EoAew4ODlfAJByDghCS4kuJJPeStjgLJ441tZ15aaHwubwbqG3P6HpDs3dCOe3uz9K/Kq/WvGqxVtQbTq2mmoprd1Rb6TdHWROkEoh5gxh27l8+F+SlwyqvnL7kw/BH4L7IwyfLK+V9SfLK+VmWCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgLpPevFsRUU80EtVG0GOH5RysUUb5pGxMGXPOAvSKI+EWSogkppnwSgB7Dg4OV91FJPStjfM0ASt3Nwc8kBgXvzLMKSd1K6sDR0TXbSc88r5pqaWrmbBCAXu7snCWDEi+nxuZI6IjxmuLT/mslVSTUcginADi0OwDnkUBhRZ46OeWmkq2NBjiOHc+aUlJNWy9BAAX4JwThAYccifmXiyNjc53RAeMXBv8ANfVVSzUcxgnaA9uDyOUBhRZ/A5/BPDdo6Ldsznnn/JKWjnrOk6BoPRt3uycckBgRegFxDR3k4CzVdJNRS9BOAH4BwDlSDAitm58WtPVtvvNJHR3IOuGmaGywl0bMNmhPjuPj8mHyEZPzgKU6cPEVvB7R8mgb9T2zZNXuq3z1cMLC3pztJ6X5QHPOAV5eXxDJhipZIablW7pcm7un2o9TB4fDiHphO3TbSVvatua7/Q/LGotQ3e3Xantdqo46iSoYwsZ0bnve9zi0NaGnmTgYHflaN71LrPTdyls2oLCy3V0GOlpqqnkjkZkAjLXOyMggqzeOd24XycajcLy651VPHaqXpKnS9RDT77k1xMkrHSMc0sznm0A7uee/MZ4k6Jpx/aOOgLhqG+3WlqbrbqGSvuNWJ658czYdxMpbguAkIGW4AA5FYx4+eScVTjqi3+Vcvz7dUUy8FHFGbTupJfmm91v2780yFdoN69FovYf8SdoN69FovYf8StC+cI+EU8uvdNaRvGq26h0RSVVwdLcPB3UdVFTyBsjGhjQ8OG5oDicE5OB3DuVnCq+cStCcIujjko7BQ2etkvF3e3EFDTtqXve5zu7dtadrfKceTJGL8XgoxnbSbW72pOMpJ/PTX73v/dWZZJYuckm6W9tSjFr/AHX8vypPtBvXotF7D/iTtBvXotF7D/iUo0XrvQ3D/Vms5rLDe5rLdrFX2W1ukMclRul2Bkkp/ZgNO0k4GRkDB71xuKOt7XrZ+lXWqCriFj0xQWWoFQ1rd08AcHuZtccsO4YJwfnAXRHi80ppaXTrf4p9PSkvmYT4bFDG5PItSbVLfk1vfrb/AC9TQ7Qb16LRew/4lG+ld8wXwi3lJy5nIekknJXiIqkhERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQF1bnAFocQD3jPevASDkHBCq+Xihf2SvYKO34BI/dv+NfPalqD0O3/ANN/xrr9pxlaLSJLiS4kk95KFznY3OJwMDJ7gqt7UtQeh2/+m/407UtQeh2/+m/409pxii0tztuzcduc4zyygJaQ5pII7iFVvalqD0O3/wBN/wAa+n8UL+1jHCjt/jAk/s3/ADn/AI09pxii0CSTknJKOc5xy5xOBjmVVvalqD0O3/03/GnalqD0O3/03/GntOMUWlucGlocdp7xnkjXOYctcQfoKq3tS1B6Hb/6b/jTtS1B6Hb/AOm/409pxii0wSGkj5wvHOc4lziST3kqsmcTr86mllNJb8scwD9m/wAuf+P6Fi7UtQeh2/8Apv8AjT2nGKLS3O27Nx25zjPLKBzm52uIyMHB7wqt7UtQeh2/+m/407UtQeh2/wDpv+NPacYotFeuc55y5xJ+kqre1LUHodv/AKb/AI07UtQeh2/+m/409pxii0VLb9q223Th9pjSdPBUtq7LLWPqHva0RuEsm5uwgknA78gfzVA9qWoPQ7f/AE3/ABp2pag9Dt/9N/xrPJkxZHFy/hdr401+priyzxalH+JU/gTKvZw+fqNx4gVeoYIBRRmnNnpoJnl/SPzv6V7ABjuxlSbXvEvhDf8AilRcV7H/AHwFwbebfXVVJV0dKyEU8HRhwYWzOcXkRjAOBknmFR141TcL3VNq6uGna9sYjAja4DAJPlJ+daPWM/ms9R964c+KGXN5tva/rV/nSNsXFSxYniUVT9N73r8rdFz0nFfTsGtuJupH0VxNNrS03SgoGCOPpIpKmRjozKN+GtAac7S4/MCt63/2gJdN0fDmLTra8jStHUUl4o5yG0twjlmc5zMBx3AxuIy5oweYHJUV1jP5rPUfenWM/ms9R96x9jwNRi1aVfROO/ykzR+IZ3OWROm73XrKMnXziiV60n0pU6ouNVoiGvgsk8xko4a2NjJoWO57CGucCGkkA55gDPNcRc/rGfzWeo+9OsZ/NZ6j710wSxxUU7rucuXI8s3NrnvsWdxP0JYtC2jRbaKqrZbtfLFFebmyZ7DHD0xPRNjAaC3xQSdxd3juUBXP6xn81nqPvTrGfzWeo+9W1K38X9+Xy5FOiXovtz+Z0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvTrGfzWeo+9NSB0EXP6xn81nqPvRNSBhqP38n8RWNZKj9/J/EVjVGAiIgCySfu4v4T/wAxWNZJP3cX8J/5iiBjREQBERAbEX+4z/8AaR//ACWutiL/AHGf/tI//ktdGAiIgCIiAv2/cftIXW1amoILdeg+86MtWnKYvhi2sqKY5kc7EhxGeeCMk+VoW5w105b/AO0Hw/smmbzc4qS48PK0GrqZpNrn6dkJfJgnvdC5pA8gD2jyr87KbcL9b2vRP97etIKuU37S9dZabwdrXbJ5tmxz9zhhg2nJGT3YBXnZuDWPHKWD8d6l13V/e2n6M9Ph+Oc5wx8R/wCOtL6bNJfSk16pMsi0WGD+0txP1XqiqprwNN6doWmjtllgY+tfSRkRUtNAx/itJGXEkEDDuXPI1eInDyv4IUemeK/D6TVump6uoqKLwW/RRMr6ScMILmuY0NfG9jnAHaMc/wCUR4McTbZoCpv1r1FDdTZdTW40FXNaZWx1tM5rg+OaEuwCQQRtJAIdnPLB+eKGsdDXy2Wmy6LOrazwN0k1XctR15fPUPdyaxsDJHRRtaM8x4xzzPLni8OfHmjih/40ku6a3u/j/T1OmPEcPl4eebJXmtt9q5adPw6Vslz2okv9pC93bUlt4ZX6+10lZcK7SMU1RUSY3SPNRLknC6Ft4ja311/Z34jUertSVd0htM1hjomTkYga6eQEDAHeGN7/AJlxLnrjhDrfROmLbrSPV1BfdL2p1pida6emmpahjXufG5xkka9p8bxsA/QuRwv1vomzaW1boXX9Le+q9TtoZBV2dsT6iCallc9o2Sua1zXbzk7sjA5HORPkf4Dx6N4zTW3TzNW38q/QnJxKlxCyrJtPGovd8/KUfe/m/qfPCzQWk7/YtVa417X3WGwaUhpemgtQj8LqZqmQxxNa6QFjW5aSSQVNKTgboHUN20fcNL3XUA09re33Y0Mda6BtZS19HHIQyRzGlj43PYMbWg4PeCo/pPXXCvTjNYaFqqfVVZorVcFH/tTY6eO5089M8yMd0e8xObvc4Eb+7Hl5Lpy8bNI2LUHDeh0Vbr23S+gauSofJcOhNdWeETB9RljD0bfFLmtbu8vMqc/tOSb8q1fLsk4Pn6qf6dLK8P7JhgvO0yrn3bU1y6aXHn/N1o6k39njStJYbBqGoul2NPNpOvvl6AmiBgqoqSCeKKP9n4rT4TECDuOM8wVr8POPWktJW/hjSXG33mQ6Lfe31/QRRES+GMkbF0WZBuxvG7dtxzxlbOpf7RmmLtpTiLYaCz3SObUlWwWN8jIw2moyynikZLh52uMdLHgNDhnPMd6/PyjDgy8VCUeKTrp/MpX+Snp/lLZ+KwcDKEuBa1bpv4aUvk9Or+Z9GgSSck5JREXrHhBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQGSo/fyfxFY1kqP38n8RWNGAiIgCySfu4v4T/zFY1kk/dxfwn/mKIGNERAEREB0bBaLjqK7UunbUYfCbjMyGITTMhYXnkNz3kNaOZ5kgLtcQeGGsOGFXQ0er6KmgfcqfwqldT1kVSySLcW7g6Jzh3g+VRRXHx4/6kcIf+6Df/XkVZvSotdXX0b/AEEN5NPtf1S/UpxERWAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQGSo/fyfxFY1kqP38n8RWNGAiIgCySfu4v4T/AMxWNS7RfCvXfEWgr63R1pjuQtQzNAyrhZUFu1zyY4nuD5OTTyYCU5Jt8kOqXciKIQQSCMEIgCIiA37DbILzeaO1VN2o7XFVTNifWVhcIIAf8by0E7R9AKtH+0DcNNutnD7TentV2y/u09p4UFXU29znQ9KJnuwC5rT3EHuVPooktSS7O/o1+rEfdbfdV9U/0QREUgIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIDJUfv5P4isayVH7+T+IrGjAREQBWz/ZYds422SVtNLI9kFbtkZs20xNLKOnfuc0bGZLnc84HLJwDUytz+zlc9J6O1eOIurtaUVrpLPHPC63GGaaruAmp5GbImsaW4y4ZLnADkj2jJ+j+1fHn237EPdpeq/f8A3sQzXujLbo6qpYrdr6waoFU173yWmSV4hII5P6RjeZzkYz3FRZeu27jsBDc8s9+FYHGe1cP7DdrDZdAyUVSKawURu1bSVrqmOpuL2bpnBxe5oxlo2twAQRjIVfwpX3r7v9K/Iu/ek0u1/VL9SvkRFYqEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAEREAREQBERAZKj9/J/EVjWSo/fyfxFY0YCIiALPb7fW3WvprXbaWSpq6yVkEEMTdz5JHEBrWjykkgBYFM+Cv/1i0N/3it3/APSxaYoLJkjB9WkUnLTFyXQ0dd8OdWcNrhBadYUVNSVk8ZlEMVbBUOYA4tIf0T3Brsg8icqNKVcVwG8UtYtaAANQXEADyf7S9RVc+KTnBSfXc1yRUZOK6BERaFQiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiIAiIgCIiAIiID/9k=)"
      ],
      "metadata": {
        "id": "IeDO3nnFaEql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INCORRECT_DATAPOINTS = ['clb0lbwzddp1s086uh1e2gw0y', 'cl8k2u1rr1gm30832fz9x3vru']\n",
        "\n",
        "def convert_img_data(img):\n",
        "  if not CONVERT_TO_LOWERCASE: return img\n",
        "\n",
        "  for label in img['Labels']:\n",
        "    label['Question'] = label['Question'].lower()\n",
        "    label['AnswerType'] = label['AnswerType'].lower()\n",
        "    label['Answer'] = [ans.lower() for ans in label['Answer']]\n",
        "  return img\n",
        "\n",
        "gt = [convert_img_data(img) for img in gt if img['ImageID'] not in INCORRECT_DATAPOINTS]"
      ],
      "metadata": {
        "id": "2XLy7t8rfnIY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset Analysis\n",
        "- **Note**: The question IDs here DO NOT correspond to the IDs in the dataset's [README](https://github.com/simula/ImageCLEFmed-MEDVQA-GI-2023#task-1-visual-question-answering-vqa)"
      ],
      "metadata": {
        "id": "7aJzCMlq2kKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GT = {\n",
        "    'question': [],\n",
        "    'answer_count': [0 for _ in range(QUESTION_COUNT)],\n",
        "    'answer_type': ['' for _ in range(QUESTION_COUNT)],\n",
        "    'is_multi_answer': [False for _ in range(QUESTION_COUNT)],\n",
        "    'possible_answers': [[] for _ in range(QUESTION_COUNT)]\n",
        "}\n",
        "\n",
        "for img in gt:\n",
        "  for label in img['Labels']:\n",
        "    q = label['Question']\n",
        "    at = label['AnswerType']\n",
        "    a = label['Answer']\n",
        "\n",
        "    if not q in GT['question']: GT['question'].append(q)\n",
        "    ix = GT['question'].index(q)\n",
        "    if GT['is_multi_answer'][ix] is False and len(a) > 1: GT['is_multi_answer'][ix] = True\n",
        "    if len(a) != 1 or a[0].lower() != 'not relevant': GT['answer_count'][ix] += 1\n",
        "    GT['answer_type'][ix] = at\n",
        "    if at.lower() == 'segmentation':\n",
        "      GT['possible_answers'][ix] = 'any mask' # TODO: Fix this\n",
        "    else:\n",
        "      GT['possible_answers'][ix] += a\n",
        "      GT['possible_answers'][ix] = list(set(GT['possible_answers'][ix]))\n",
        "\n",
        "GT = pd.DataFrame(GT)\n",
        "\n",
        "GT = GT.sort_values(by='answer_count', ascending=False)\n",
        "GT = GT.sort_values(by='possible_answers', key=lambda x: x.str.len())\n",
        "GT = GT.sort_values(by='answer_type', key=lambda x: x.eq('segmentation'))\n",
        "GT = GT.reset_index(drop=True)\n",
        "\n",
        "GT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "ybPCSua6IAGc",
        "outputId": "2e45e204-0222-41dc-aca5-2a4c9d90a113"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             question  answer_count  \\\n",
              "0                                      is there text?          1998   \n",
              "1      where in the image is the anatomical landmark?           522   \n",
              "2               where in the image is the instrument?           555   \n",
              "3                      how many findings are present?          1998   \n",
              "4                   how many polyps are in the image?          1998   \n",
              "5              what color is the anatomical landmark?            88   \n",
              "6             are there any instruments in the image?          1014   \n",
              "7              where in the image is the abnormality?          1613   \n",
              "8    are there any anatomical landmarks in the image?          1566   \n",
              "9           are there any abnormalities in the image?          1998   \n",
              "10             how many instrumnets are in the image?          1998   \n",
              "11                     what type of polyp is present?           613   \n",
              "12                      have all polyps been removed?           629   \n",
              "13                    is this finding easy to detect?          1982   \n",
              "14               is there a green/black box artefact?          1998   \n",
              "15    what type of procedure is the image taken from?          1998   \n",
              "16                     what is the size of the polyp?           616   \n",
              "17                     what color is the abnormality?          1628   \n",
              "18   where exactly in the image is the polyp located?           499   \n",
              "19  where exactly in the image is the instrument l...           182   \n",
              "\n",
              "     answer_type  is_multi_answer  \\\n",
              "0         yes/no            False   \n",
              "1           text             True   \n",
              "2           text             True   \n",
              "3         number            False   \n",
              "4         number            False   \n",
              "5           text             True   \n",
              "6           text             True   \n",
              "7           text             True   \n",
              "8           text            False   \n",
              "9           text             True   \n",
              "10        number            False   \n",
              "11          text             True   \n",
              "12        yes/no            False   \n",
              "13        yes/no            False   \n",
              "14        yes/no            False   \n",
              "15          text            False   \n",
              "16          text             True   \n",
              "17          text             True   \n",
              "18  segmentation             True   \n",
              "19  segmentation             True   \n",
              "\n",
              "                                     possible_answers  \n",
              "0                                           [yes, no]  \n",
              "1   [upper-left, upper-center, lower-rigth, center...  \n",
              "2   [lower-right, upper-center, upper-left, center...  \n",
              "3                           [2, 4, 0, 1, 6, 5, 16, 3]  \n",
              "4                           [2, 4, 0, 1, 6, 5, 16, 3]  \n",
              "5   [grey, brown, red, not relevant, yellow, white...  \n",
              "6   [tube, biopsy forceps, polyp snare, not releva...  \n",
              "7   [lower-right, upper-left, upper-center, center...  \n",
              "8   [z-line, ileum, not relevant, cecum, no, pylorus]  \n",
              "9   [oesophagitis, barretts, ulcerative colitis, p...  \n",
              "10                                       [1, 2, 0, 3]  \n",
              "11      [not relevant, paris iia, paris ip, paris is]  \n",
              "12                            [yes, no, not relevant]  \n",
              "13                            [yes, no, not relevant]  \n",
              "14                                          [yes, no]  \n",
              "15                         [gastroscopy, colonoscopy]  \n",
              "16      [11-20mm, >20mm, < 5mm, not relevant, 5-10mm]  \n",
              "17  [grey, brown, blue, red, burgundy, violet, bla...  \n",
              "18                                           any mask  \n",
              "19                                           any mask  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cc7d5b5d-3d16-4ba4-ab37-a4f28809e926\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer_count</th>\n",
              "      <th>answer_type</th>\n",
              "      <th>is_multi_answer</th>\n",
              "      <th>possible_answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>is there text?</td>\n",
              "      <td>1998</td>\n",
              "      <td>yes/no</td>\n",
              "      <td>False</td>\n",
              "      <td>[yes, no]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>where in the image is the anatomical landmark?</td>\n",
              "      <td>522</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[upper-left, upper-center, lower-rigth, center...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>where in the image is the instrument?</td>\n",
              "      <td>555</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[lower-right, upper-center, upper-left, center...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how many findings are present?</td>\n",
              "      <td>1998</td>\n",
              "      <td>number</td>\n",
              "      <td>False</td>\n",
              "      <td>[2, 4, 0, 1, 6, 5, 16, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>how many polyps are in the image?</td>\n",
              "      <td>1998</td>\n",
              "      <td>number</td>\n",
              "      <td>False</td>\n",
              "      <td>[2, 4, 0, 1, 6, 5, 16, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>what color is the anatomical landmark?</td>\n",
              "      <td>88</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[grey, brown, red, not relevant, yellow, white...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>are there any instruments in the image?</td>\n",
              "      <td>1014</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[tube, biopsy forceps, polyp snare, not releva...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>where in the image is the abnormality?</td>\n",
              "      <td>1613</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[lower-right, upper-left, upper-center, center...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>are there any anatomical landmarks in the image?</td>\n",
              "      <td>1566</td>\n",
              "      <td>text</td>\n",
              "      <td>False</td>\n",
              "      <td>[z-line, ileum, not relevant, cecum, no, pylorus]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>are there any abnormalities in the image?</td>\n",
              "      <td>1998</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[oesophagitis, barretts, ulcerative colitis, p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>how many instrumnets are in the image?</td>\n",
              "      <td>1998</td>\n",
              "      <td>number</td>\n",
              "      <td>False</td>\n",
              "      <td>[1, 2, 0, 3]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>what type of polyp is present?</td>\n",
              "      <td>613</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[not relevant, paris iia, paris ip, paris is]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>have all polyps been removed?</td>\n",
              "      <td>629</td>\n",
              "      <td>yes/no</td>\n",
              "      <td>False</td>\n",
              "      <td>[yes, no, not relevant]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>is this finding easy to detect?</td>\n",
              "      <td>1982</td>\n",
              "      <td>yes/no</td>\n",
              "      <td>False</td>\n",
              "      <td>[yes, no, not relevant]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>is there a green/black box artefact?</td>\n",
              "      <td>1998</td>\n",
              "      <td>yes/no</td>\n",
              "      <td>False</td>\n",
              "      <td>[yes, no]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>what type of procedure is the image taken from?</td>\n",
              "      <td>1998</td>\n",
              "      <td>text</td>\n",
              "      <td>False</td>\n",
              "      <td>[gastroscopy, colonoscopy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>what is the size of the polyp?</td>\n",
              "      <td>616</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[11-20mm, &gt;20mm, &lt; 5mm, not relevant, 5-10mm]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>what color is the abnormality?</td>\n",
              "      <td>1628</td>\n",
              "      <td>text</td>\n",
              "      <td>True</td>\n",
              "      <td>[grey, brown, blue, red, burgundy, violet, bla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>where exactly in the image is the polyp located?</td>\n",
              "      <td>499</td>\n",
              "      <td>segmentation</td>\n",
              "      <td>True</td>\n",
              "      <td>any mask</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>where exactly in the image is the instrument l...</td>\n",
              "      <td>182</td>\n",
              "      <td>segmentation</td>\n",
              "      <td>True</td>\n",
              "      <td>any mask</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc7d5b5d-3d16-4ba4-ab37-a4f28809e926')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cc7d5b5d-3d16-4ba4-ab37-a4f28809e926 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cc7d5b5d-3d16-4ba4-ab37-a4f28809e926');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-289d08d7-72d7-4a67-8eb9-181204cda2ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-289d08d7-72d7-4a67-8eb9-181204cda2ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-289d08d7-72d7-4a67-8eb9-181204cda2ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Combining questions"
      ],
      "metadata": {
        "id": "wYIVGv_o8YaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Combine similar questions here"
      ],
      "metadata": {
        "id": "-PsD6WZx8aJh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectoring the images using VGG (Run only once) - Follow [this](https://medium.com/@rajeshmane711/visual-question-answering-system-using-deep-learning-techniques-5636a9c6b72d) article"
      ],
      "metadata": {
        "id": "viWQqk_Rgb8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# # Load ResNet50 model with pre-trained weights on ImageNet\n",
        "# model = ResNet50(weights='imagenet')\n",
        "# # Remove the final classification layer\n",
        "# feature_model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "# def parse_function(filename):\n",
        "#     image_string = tf.io.read_file(filename)\n",
        "#     image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "#     image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "#     image = tf.image.resize(image, [224, 224])  # Assuming ResNet50 input size\n",
        "#     return image, filename\n",
        "\n",
        "\n",
        "\n",
        "# img_fl = sorted(set([f\"{BASE_PATH}/dataset/dev/images/{img['ImageID']}.jpg\" for img in gt]))\n",
        "# print(len(img_fl))\n",
        "# img_data_tr = tf.data.Dataset.from_tensor_slices(img_fl)\n",
        "# img_data_tr = img_data_tr.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# for img, path in tqdm(img_data_tr):\n",
        "#     batch_features = feature_model(img)\n",
        "\n",
        "#     for bf, p in zip(batch_features, path):\n",
        "#         path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#         path_of_feature = path_of_feature.replace(\"dataset/dev/images\", \"out/trial1\").replace(\".jpg\", \"\")\n",
        "#         np.save(path_of_feature, bf.numpy())\n",
        "\n",
        "\n",
        "# sync_gdrive()\n"
      ],
      "metadata": {
        "id": "E-diJ7ftMRxL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = VGG16(weights='imagenet')\n",
        "# feature_model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "\n",
        "# def parse_function(filename):\n",
        "#     image_string = tf.io.read_file(filename)\n",
        "#     #Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "#     image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "#     #This will convert to float values in [0, 1]\n",
        "#     image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "#     image = tf.image.resize(image, [224, 224])\n",
        "#     return  image, filename\n",
        "\n",
        "# img_fl = sorted(set([f\"{BASE_PATH}/dataset/dev/images/{img['ImageID']}.jpg\" for img in gt]))\n",
        "# img_data_tr = tf.data.Dataset.from_tensor_slices(img_fl)\n",
        "# img_data_tr = img_data_tr.map(parse_function,num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# for img, path in tqdm(img_data_tr):\n",
        "#   batch_features = feature_model(img)\n",
        "#   #batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "#   for bf, p in zip(batch_features, path):\n",
        "#     path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "#     path_of_feature = path_of_feature.replace(\"dataset/dev/images\", \"out/trial1\").replace(\".jpg\", \"\")\n",
        "#     np.save(path_of_feature, bf.numpy())\n",
        "\n",
        "# sync_gdrive()"
      ],
      "metadata": {
        "id": "SFhXqsKwMBug"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creating and splitting the dataset"
      ],
      "metadata": {
        "id": "58vfWb403RWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_to_include = GT['question'][:QUESTIONS_TO_BE_INCLUDED].tolist()\n",
        "# possible_answers = {q: GT.loc[GT['question'] == q, 'possible_answers'].values[0] for q in questions_to_include}\n",
        "# possible_answers = [element for row in possible_answers.values() for element in row]\n",
        "\n",
        "dataset = {\n",
        "    'image': [],\n",
        "    'question': [],\n",
        "    'answer': []\n",
        "}\n",
        "\n",
        "for img in gt:\n",
        "  for label in img['Labels']:\n",
        "    if label['Question'] not in questions_to_include: continue\n",
        "\n",
        "    dataset['image'].append(img['ImageID'])\n",
        "    dataset['question'].append(label['Question'])\n",
        "    dataset['answer'].append(','.join(label['Answer'])) # Join answers\n",
        "\n",
        "dataset = pd.DataFrame(dataset)\n",
        "\n",
        "X = dataset[['question', 'image']]\n",
        "y = dataset['answer']\n",
        "\n",
        "print(f\"Before Split - {len(X)}, {len(y)}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, shuffle=True, random_state=42)\n",
        "\n",
        "print(f\"Train - {len(X_train)}, {len(y_train)}\")\n",
        "print(f\"TEst - {len(X_test)}, {len(y_test)}\")"
      ],
      "metadata": {
        "id": "WgpDUpFByK94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addd6461-9c49-4c90-847b-d2602b2c3c0b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Split - 35964, 35964\n",
            "Train - 25174, 25174\n",
            "TEst - 10790, 10790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectorizing the questions"
      ],
      "metadata": {
        "id": "yQy1hpPi3nmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "glove_model = api.load(WORD_VECTOR_MODEL)\n",
        "\n",
        "max_length = GT['question'].str.len().max()\n",
        "\n",
        "def textokenizer(text):\n",
        "  tok = Tokenizer()\n",
        "  tok.fit_on_texts(X_train['question'])\n",
        "  vocab_size = len(tok.word_index) + 1\n",
        "  print('Total unique words in the X_train',vocab_size)\n",
        "  encoded_text = tok.texts_to_sequences(text)\n",
        "  padded_text = pad_sequences(encoded_text, maxlen=max_length)  #padding zeros at the begining of each question so \\\n",
        "                                                                #that each sequence will have same length\n",
        "  return padded_text, tok\n",
        "\n",
        "# for train\n",
        "padded_text, tok = textokenizer(X_train['question'])\n",
        "vocab_size = len(tok.word_index) + 1\n",
        "embedding_matrix_train = np.zeros((vocab_size, 300))\n",
        "for word, i in tok.word_index.items():\n",
        "    if word in glove_model:\n",
        "        embedding_vector = glove_model[word]\n",
        "        embedding_matrix_train[i] = embedding_vector"
      ],
      "metadata": {
        "id": "BexSvvsQnAUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26aabb4-5a54-4afc-a240-c6f652fa8788"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
            "Total unique words in the X_train 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- One hot encoding"
      ],
      "metadata": {
        "id": "7HPSJZHu3yoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optokens(classes):\n",
        "  from sklearn.preprocessing import OneHotEncoder\n",
        "  ohe=OneHotEncoder(handle_unknown='ignore')\n",
        "  ohe.fit(y_train.values.reshape(-1,1))\n",
        "  optoken=ohe.transform(classes.values.reshape(-1,1)).toarray()\n",
        "  return optoken, ohe\n",
        "\n",
        "_,ohe=optokens(y_train)"
      ],
      "metadata": {
        "id": "8rlBCCD-7FeL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vectorize the dataset"
      ],
      "metadata": {
        "id": "KUddW2Cs38j3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ques_train , _ =textokenizer(X_train['question'])\n",
        "answ_train , _ =optokens(y_train)\n",
        "\n",
        "ques_train=ques_train.tolist()\n",
        "answ_train=answ_train.tolist()\n",
        "\n",
        "\n",
        "ques_test,_=textokenizer(X_test['question'])\n",
        "answ_test,_=optokens(y_test)\n",
        "\n",
        "ques_test=ques_test.tolist()\n",
        "answ_test=answ_test.tolist()"
      ],
      "metadata": {
        "id": "We-QbOORLmr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75e340b-fab3-4b9d-e7c8-07cc28c1a92d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words in the X_train 46\n",
            "Total unique words in the X_train 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Combining images and questions"
      ],
      "metadata": {
        "id": "4Nuo45x04HMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(img_name, question):\n",
        "  img_tensor = np.load(BASE_PATH + '/out/trial1/' + img_name.decode('utf-8') + '.npy')\n",
        "  return img_tensor, question\n",
        "\n",
        "def map_main_fn(item1, item2):\n",
        "  return tf.numpy_function(map_func, [item1, item2], [tf.float32, tf.int32])\n",
        "\n",
        "def shape(a, b):\n",
        "  a.set_shape(IMG_DIMN)\n",
        "  b.set_shape(max_length)\n",
        "  return a,b\n",
        "\n",
        "img_fl = X_train['image'].tolist()\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((img_fl, ques_train))\n",
        "\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "train_dataset = train_dataset.map(map_main_fn, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.map(shape)\n",
        "op_train_dataset = tf.data.Dataset.from_tensor_slices(answ_train)\n",
        "train_dataset = tf.data.Dataset.zip((train_dataset,op_train_dataset))\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "img_fl_te = X_test['image'].tolist()\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((img_fl_te, ques_test))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "test_dataset = test_dataset.map(map_main_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.map(shape)\n",
        "op_test_dataset = tf.data.Dataset.from_tensor_slices(answ_test)\n",
        "test_dataset = tf.data.Dataset.zip((test_dataset,op_test_dataset))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "1vNBVbLO7FwZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Creating the fusion model"
      ],
      "metadata": {
        "id": "Qxm4Egmy4Psl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "#IMAGE MODEL\n",
        "im_input = Input(shape=(4096,), name = \"im_input\")\n",
        "#flat = Flatten()(im_input)\n",
        "image_model=Dense(1024,activation='relu',kernel_initializer=initializers.he_normal(seed=42))(im_input)\n",
        "image_model=Model(inputs=im_input,outputs=image_model)\n",
        "image_model.summary()\n",
        "\n",
        "#QUESTION MODEL\n",
        "ques_input = Input(shape=(max_length,), name = \"ques_input\")\n",
        "e1 =Embedding(vocab_size, 300, weights=[embedding_matrix_train], input_length=max_length,trainable=False)(ques_input)\n",
        "l1= LSTM(64,kernel_initializer=initializers.he_normal(seed=42),kernel_regularizer=l2(0.001),return_sequences=True)(e1)\n",
        "l2= LSTM(64,kernel_initializer=initializers.he_normal(seed=42),kernel_regularizer=l2(0.001),return_sequences=True)(l1)\n",
        "#l1= LeakyReLU(alpha = 0.3)(l1)\n",
        "f1= Flatten(name='flatten_1')(l2)\n",
        "question_model=Dense(1024,activation='relu',kernel_initializer=initializers.he_normal(seed=42))(f1)\n",
        "question_model = Model(inputs=ques_input, outputs=question_model)\n",
        "question_model.summary()\n",
        "\n",
        "#COMBINING FEATURES AND MAKING FINAL MODEL FOR PREDICTION\n",
        "input_model=multiply([image_model.layers[-1].output,question_model.layers[-1].output])\n",
        "d1=BatchNormalization()(input_model)\n",
        "d1 = Dropout(0.5)(d1)\n",
        "d1=Dense(1000,activation='relu',kernel_initializer=initializers.he_normal(seed=42))(d1)\n",
        "final_output = Dense(op_train_dataset.element_spec.shape[0], kernel_initializer=initializers.he_normal(seed=42),activation='softmax')(d1)\n",
        "final_model = Model(inputs=[im_input,ques_input], outputs=final_output)\n",
        "print(final_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuJSpFhU9Aaw",
        "outputId": "5abc7398-d776-463b-f5b1-e9a2bf62727a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " im_input (InputLayer)       [(None, 4096)]            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              4195328   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4195328 (16.00 MB)\n",
            "Trainable params: 4195328 (16.00 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " ques_input (InputLayer)     [(None, 53)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 53, 300)           13800     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 53, 64)            93440     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 53, 64)            33024     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 3392)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              3474432   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3614696 (13.79 MB)\n",
            "Trainable params: 3600896 (13.74 MB)\n",
            "Non-trainable params: 13800 (53.91 KB)\n",
            "_________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " ques_input (InputLayer)     [(None, 53)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 53, 300)              13800     ['ques_input[0][0]']          \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 53, 64)               93440     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 53, 64)               33024     ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " im_input (InputLayer)       [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 3392)                 0         ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1024)                 4195328   ['im_input[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1024)                 3474432   ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 1024)                 0         ['dense[0][0]',               \n",
            "                                                                     'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 1024)                 4096      ['multiply[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 1024)                 0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1000)                 1025000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 340)                  340340    ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9179460 (35.02 MB)\n",
            "Trainable params: 9163612 (34.96 MB)\n",
            "Non-trainable params: 15848 (61.91 KB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Training"
      ],
      "metadata": {
        "id": "KDXPNPtb4Wuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vaukpi6d53E",
        "outputId": "9de6c4f1-4f29-4c91-cf64-db7833e75882"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3147"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uzxTqCnd_jK",
        "outputId": "f0e562da-c669-4533-d1b2-71ed13a023ec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1349"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint(MODEL_PATH,\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "earlystop= EarlyStopping(monitor = 'val_loss',\n",
        "                            mode=\"min\",\n",
        "                            min_delta = 0,\n",
        "                            patience = 10,\n",
        "                            verbose = 1)\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='logs1',histogram_freq=1,write_grads=True)\n",
        "callbacks = [checkpoint,earlystop,tensorboard]\n",
        "\n",
        "if USE_CHECKPOINT: final_model=load_model(MODEL_PATH)\n",
        "else:\n",
        "    final_model.compile(optimizer=Adam(learning_rate=LR), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Print model summary\n",
        "    final_model.summary()\n",
        "\n",
        "    # Take a single batch for inspection\n",
        "    for img, label in train_dataset.take(1):\n",
        "        # Print the shapes of input data\n",
        "        #print(\"Input Image Shape:\", img.shape)\n",
        "        print(\"Input Label Shape:\", label.shape)\n",
        "\n",
        "        # Make predictions on the training batch\n",
        "        predictions = final_model.predict(img)\n",
        "\n",
        "        # Print the shape of the model predictions\n",
        "        print(\"Model Predictions Shape:\", predictions.shape)\n",
        "\n",
        "    # Train the model\n",
        "    h1 = final_model.fit(train_dataset, epochs=EPOCHS-5, verbose=1, callbacks=callbacks, validation_data=test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0LnMbvOZ9bmD",
        "outputId": "fcb554cc-ad92-4328-feb9-d7bd835edded"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " ques_input (InputLayer)     [(None, 53)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 53, 300)              13800     ['ques_input[0][0]']          \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 53, 64)               93440     ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 53, 64)               33024     ['lstm[0][0]']                \n",
            "                                                                                                  \n",
            " im_input (InputLayer)       [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 3392)                 0         ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1024)                 4195328   ['im_input[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 1024)                 3474432   ['flatten_1[0][0]']           \n",
            "                                                                                                  \n",
            " multiply (Multiply)         (None, 1024)                 0         ['dense[0][0]',               \n",
            "                                                                     'dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 1024)                 4096      ['multiply[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 1024)                 0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 1000)                 1025000   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 340)                  340340    ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9179460 (35.02 MB)\n",
            "Trainable params: 9163612 (34.96 MB)\n",
            "Non-trainable params: 15848 (61.91 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes at component 0: expected [?,4096] but got [8,2048]. [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7c7238d890d8>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Take a single batch for inspection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Print the shapes of input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#print(\"Input Image Shape:\", img.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    774\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5882\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5883\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes at component 0: expected [?,4096] but got [8,2048]. [Op:IteratorGetNext] name: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- function to run the trained model for a given input"
      ],
      "metadata": {
        "id": "EqUM7UYO4Y9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_answer(filename,question,answer):\n",
        "  def textokenizer(text):\n",
        "     text=[text]\n",
        "     encoded_text = tok.texts_to_sequences(text)\n",
        "     padded_text = pad_sequences(encoded_text, maxlen=max_length)  #padding zeros at the begining of each question so that each sequence will have same length\n",
        "     return  padded_text\n",
        "\n",
        "  def optokens(answer):\n",
        "     global ohe\n",
        "     answer=np.array(answer,dtype=object)\n",
        "     #ohe=pickle.load(open('/content/drive/My Drive/ohe.pkl','rb'))\n",
        "     optoken=ohe.transform(answer.reshape(-1,1)).toarray()\n",
        "     return optoken, ohe\n",
        "\n",
        "  def parse_function(filename):\n",
        "    image_string = tf.io.read_file(BASE_PATH + '/dataset/dev/images/' + filename  + '.jpg')\n",
        "    #Don't use tf.image.decode_image, or the output shape will be undefined\n",
        "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    #This will convert to float values in [0, 1]\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    plt.imshow(image)\n",
        "    image =tf. expand_dims(image, axis=0)\n",
        "    return  image, filename\n",
        "\n",
        "  model = ResNet50(weights='imagenet')\n",
        "  feature_model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "  image,_=parse_function(filename)\n",
        "  image_feature = feature_model(image)\n",
        "  question_feature = textokenizer(question)\n",
        " # answer_feature,_ = optokens(answer)\n",
        "  answer_vector = final_model.predict([image_feature,question_feature])\n",
        "  _,ohe=optokens(answer)\n",
        "  predicted_answer=ohe.inverse_transform(answer_vector)\n",
        "  predicted_answer=predicted_answer.tolist()[0][0]\n",
        "  return predicted_answer"
      ],
      "metadata": {
        "id": "4v8IX49Y0HLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Demo"
      ],
      "metadata": {
        "id": "lueabXj54gze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = X_test.sample(n=DEMO_COUNT)\n",
        "actual_outputs = y_test.loc[inputs.index]\n",
        "\n",
        "plt.figure(figsize=(40, 20))\n",
        "plt.tight_layout()\n",
        "columns = 3\n",
        "\n",
        "for i, (index, input) in enumerate(inputs.iterrows()):\n",
        "  predicted_ans = predict_answer(input['image'], input['question'], actual_outputs.loc[index])\n",
        "\n",
        "  plt.subplot(DEMO_COUNT // columns + 1, columns, i + 1)\n",
        "  img = mpimg.imread(BASE_PATH + '/dataset/dev/images/' + input['image']  + '.jpg')\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Question: {input['question']}\\nActual answer: {actual_outputs.loc[index]}\\nPredicted answer: {predicted_ans}\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94drhVKYiQc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmTqI82ScB-n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}